[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "anatomy/vm.html",
    "href": "anatomy/vm.html",
    "title": "Virtual Machines",
    "section": "",
    "text": "As a user of FlowEHR you will need to create a virtual machine (VM) within an Azure Workspace in order to access FlowEHR assets within a secure Trusted Research Environment (TRE). The following instructions describe the process of VM creation and access, as well as some initial setup to access a Jupyter notebook once inside the VM.\nThis documentation relies heavily upon the official AzureTRE docs which should be consulted before performing the below steps."
  },
  {
    "objectID": "anatomy/vm.html#creating-a-new-vm",
    "href": "anatomy/vm.html#creating-a-new-vm",
    "title": "Virtual Machines",
    "section": "1 Creating a new VM",
    "text": "1 Creating a new VM\n\nVisit the FlowEHR Azure TRE landing page URL (please request from a member of the team)\nSelect an appropriate Workspace from the list provided\nUnder Workspace Services, select the VMs option\nUnder Resources, you will see any previously created VMs. If there are none, select Create New in the upper right\nSelect Linux Machine &gt; Create (only option available as of 05/12/2022)\nChoose an approriate name and description for the VM and select the desired image.\n\nFor linux VMs, there are two Ubuntu 18.04 images available. The Data Science variant may have a more relevant selection of packages installed.\n\nSelect an approriate VM size from the dropdown menu. If you’re not sure of your requirements, opt for the 2 CPU | 8GB RAM option. You can scale this up later if needed.\nCheck the box marked Shared storage to enable access to workspace shared storage.\nHit Submit. Your VM will start provisioning and you may need to wait a few minutes."
  },
  {
    "objectID": "anatomy/vm.html#stopping-a-running-vm",
    "href": "anatomy/vm.html#stopping-a-running-vm",
    "title": "Virtual Machines",
    "section": "2 Stopping a running VM",
    "text": "2 Stopping a running VM\n\nSelect the three dots at the upper right of the VM entry under Resources\nSelect Action &gt; Stop\nThe VM will be stopped momentarily"
  },
  {
    "objectID": "anatomy/vm.html#starting-a-stopped-vm",
    "href": "anatomy/vm.html#starting-a-stopped-vm",
    "title": "Virtual Machines",
    "section": "3 Starting a stopped VM",
    "text": "3 Starting a stopped VM\n\nSelect the three dots at the upper right of the VM entry under Resources\nSelect Action &gt; Start"
  },
  {
    "objectID": "anatomy/vm.html#connecting-to-a-created-vm",
    "href": "anatomy/vm.html#connecting-to-a-created-vm",
    "title": "Virtual Machines",
    "section": "4 Connecting to a created VM",
    "text": "4 Connecting to a created VM\n\nVisit the FlowEHR Azure TRE landing page URL\nSelect an appropriate Workspace from the list provided\nUnder Workspace Services, select the VMs option\nUnder Resources, you will see any previously created VMs\nSelect Connect under the VM you wish to connect to. This launches a separate browser window or tab."
  },
  {
    "objectID": "anatomy/vm.html#deleting-a-vm",
    "href": "anatomy/vm.html#deleting-a-vm",
    "title": "Virtual Machines",
    "section": "5 Deleting a VM",
    "text": "5 Deleting a VM\n\nSelect the three dots at the upper right of the VM entry under Resources\nSelect Action &gt; Stop\nWhen the VM has been deallocated, repeat step 1. and select Delete"
  },
  {
    "objectID": "anatomy/vm.html#installing-software-within-the-vm",
    "href": "anatomy/vm.html#installing-software-within-the-vm",
    "title": "Virtual Machines",
    "section": "6 Installing software within the VM",
    "text": "6 Installing software within the VM\nAs the VM is located within a TRE, most outbound internet access is restricted.\n\n6.1 GitHub\nAccess to repositories hosted on GitHub is restricted. Repositories can be mirrored on an ad-hoc basis to a TRE-accessible Gitea instance, where this documentation can also be found.\nYou should be supplied with the URL for the Gitea instance during your onboarding. If you have not received the Gitea URL, please request it from member of the team.\n\n\n6.2 APT packages\nThe VM has access to software packages via a Nexus mirror\n\n\n6.3 Python packages via Conda and PyPI\nThe Nexus mirror provides mirrored access to packages available via conda forge and PyPI via the standard pip install and conda install command line interfaces."
  },
  {
    "objectID": "anatomy/vm.html#accessing-ehr-and-dicom-data-within-the-tre",
    "href": "anatomy/vm.html#accessing-ehr-and-dicom-data-within-the-tre",
    "title": "Virtual Machines",
    "section": "7 Accessing EHR and DICOM data within the TRE",
    "text": "7 Accessing EHR and DICOM data within the TRE\nThis documentation is accompanied by Jupyter notebook files which provide detailed examples of accessing and viewing EHR and DICOM data from within a Jupyter Lab environment within a TRE VM.\nBefore launching the notebooks and after following the steps in the above example, install required dependencies within your created virtual environment with\npip install -r requirements.txt\n\n7.1 A note on the data accessible from the TRE\nData provided has gone through an anonymisation step and will not entirely look like the original data due to the removal of PII and consequent structural changes that may appear as a result, such as line break removals.\nAs such, structural elements of reports such as line breaks or sentence lengths should not be used to generate features as inputs to machine learning models."
  },
  {
    "objectID": "anatomy/tre.html",
    "href": "anatomy/tre.html",
    "title": "Trusted Research Environment",
    "section": "",
    "text": "The FlowEHR TRE (Trusted Research Environment) provides a safe and secure environment to run clinical research on real data generated in clinical settings. The FlowEHR TRE builds upon the work of the Microsoft Azure TRE to provide access to cloud-scale computing environments to conduct real-world Machine Learning experiments from initial inception through to operational deployment.\nTRE Workspaces are designed to be project-specific, with access to the Workspace resources and Workspace data restricted to the users assigned to the Workspace. Users from other Workspaces will not be able to see or interact with your Workspace resources or data."
  },
  {
    "objectID": "anatomy/tre.html#getting-started",
    "href": "anatomy/tre.html#getting-started",
    "title": "Trusted Research Environment",
    "section": "1 Getting Started",
    "text": "1 Getting Started\nYour TRE administrator will provide you with the following information: 1) Account details to logon and access TRE resources 2) The address for the TRE Portal 3) One or more Workspace IDs"
  },
  {
    "objectID": "anatomy/tre.html#the-tre-landing-page",
    "href": "anatomy/tre.html#the-tre-landing-page",
    "title": "Trusted Research Environment",
    "section": "2 The TRE Landing Page",
    "text": "2 The TRE Landing Page\nTRE Workspaces are deployed to a virtual private network and the resources available in the Workspace are designed to have no direct internet access and are not accessible from the internet either.\nTo get access to your TRE Workspace, you must first logon to the TRE portal. Your TRE administrator will provide you with the URL to access the portal along with a logon id that grants you access to Workspace resources.\nOnce logged-in, you will see the TRE Landing Page:\n\nThe main components of the Landing Page are:\n\nThe TRE Header\nThe TRE Navigation Side-Panel\nThe ‘Workspaces’ Pane\nThe TRE Footer\n\n\n2.1 The TRE Header\n\nThe TRE Header contains the following links:\n\nThe home link - a half-full (we’re optimists) beaker named ‘Azure TRE’. Use this link to return to the Landing Page from any location within the TRE application.\nA notifications bell - an icon which can be used for quick access to any notifications you may have\nAn account link - you can use this link to logout of the TRE\n\n\n\n2.2 The Navigation Side-Panel\n\nThe Navigation Side-Panel provides context-sensitive navigation within the TRE application.\n\n\n2.3 The Workspace Pane\n\nThe ‘Workspaces’ Pane contains Card(s) for the Workspaces that you have been granted access to in the TRE.\nEach Card displays:\n\nThe Workspace Name\nDescription for the Workspace\nAn information button\nA Cost-Notifier button: displays the accumulated costs for the workspace\n\nClicking the Workspace Name or a blank space on a Workspace Card will connect you to the Workspace."
  },
  {
    "objectID": "anatomy/tre.html#using-a-workspace",
    "href": "anatomy/tre.html#using-a-workspace",
    "title": "Trusted Research Environment",
    "section": "3 Using a Workspace",
    "text": "3 Using a Workspace\nOnce connected to a workspace, you’ll be able to see the services installed in the Workspace and also the Shared Services available to all Workspaces.\n\nThe ‘Create New’ button can be used to add services to the Workspace. Only TRE Administrators or Workspace Owners can add Services to a Workspace.\nHowever, certain Workspace Services provide User Resources which enable all workspace users to add these resources to the Workspace. Specifically, the Virtual Machines Service provides a Virtual Machine User Resource. Once the Workspace Owner has added a Virtual Machines Service to the Workspace, users can connect to the service and add Virtual Machines for their personal use.\nFrom the Workspace Overview page, you can find important information about your workspace by clicking the details tab. One key piece of information here is the ‘Workspace id’. This is a four-character code, that is appended to the names of the resources deployed in your workspace. Workspace ID can be useful to know when you are connecting to resources such as AMLS\n\n3.1 Access a Virtual Machine\nLook for the Virtual Machines or Virtual Desktops Service in the Services section of your workspace:\n\nAvoid the ‘connect’ button and instead click on the title of the Virtual Machines Service Card. This will take you to a page showing all the Virtual Machines that you have access to:\n\nThe Virtual Machines in your Workspace allow you to interact with the private resources in the Workspace and with the Shared Services common to all workspaces. When you connect to a VM that you have created, you will be automatically logged-in with a user account with administrator rights. So, on a Linux VM, you will be able to run privileged commands via sudo.\nDetailed instructions for using a VM can be found in the Accessing Virtual Machines document\n\n\n3.2 Azure Machine Learning Services (AMLS)\nThe Azure Machine Learning Services in your workspace are provisioned in a private virtual network - there is no direct access to the service over the internet. Because the Virtual Machines are deployed to the same private network as AMLS, you can connect to AMLS from within a Virtual Machine.\nYou’ll need to know the URL to use for connecting. Click on the Azure Machine Learning service in your workspace then select the details tab. Towards the bottom left of this page you will see the value for the ‘internal connection url’ which you can use to access the AMLS service from your Virtual Machine.\n\nAn alternative to copy and pasting this URL, involves using your Workspace ID (which you can find in the via Workspace &gt; Overview &gt; Details). Having logged-on to your workspace VM, navigate to ‘https://ml.azure.com’ and sign-in using the same credentials that you use to access the TRE. Then navigate to ‘Workspaces’ and select the workspace corresponding to your TRE Workspace ID (the resource group for the workspace contains the workspace id as its last four characters):\n\nOnce connected to the AML Workspace, you will need to create a Compute Instance to access Jupyter or run a terminal shell. Whilst you can do this from within the AML Workspace, you can also add a compute instance using the TRE Portal:\n\nNavigate to the Azure Machine Learning service in the TRE Portal and select ‘Create New’ in the ‘Resources’ section.\nClick ‘Create’ on the ‘Azure Machine Learning Compute Instance’ item\nYou can accept the defaults for most of the fields, although you may want to change the ‘Name for the user resource’ fields\nThe final field requires your Azure Active Directory User Object-ID, you can find this by running the following commands:\n\naz login --tenant &lt;tenant name&gt;\naz ad signed-in-user show --query id -o tsv"
  },
  {
    "objectID": "anatomy/tre.html#shared-services",
    "href": "anatomy/tre.html#shared-services",
    "title": "Trusted Research Environment",
    "section": "4 Shared Services",
    "text": "4 Shared Services\nShared Services provide access to resources that you would otherwise need to access over the internet. Shared Services include:\n\nGitea\nNexus\nFirewallhttps://learn.microsoft.com/en-us/cli/azure/install-azure-cli\n\nThe Shared Services are available for use by all workspaces, but their configuration can only be changed by the TRE administrators\n\n4.1 Gitea\nGitea is used in the TRE to mirror code repositories from the internet. You can browse the repositories available in Gitea from your Workspace VM using the URL https://gitea-[TRE-ID].azurewebsites.net. (Check with your TRE administrator for the correct URL)\nContact your TRE administrator if you need to add a git repository to the Gitea service.\nGitea is also available as a Workspace service, this is useful if you wish to mirror a private repository, without making this accessible from other Workspaces.\nWhilst Gitea is used to mirror repositories from the internet, for security reasons, the server does not allow you to push changes to internet sources.\n\n\n4.2 Nexus\nNexus is used in the TRE to provide a package mirror for popular software tools. Installation tools on your VMs are configured to pull from the Nexus server rather than directly from the internet.\nYou can explore the available repositories mirrored by the Nexus server from your VM using https://nexus-[TRE-ID].[LOCATION].cloudapp.azure.com. (Check with your TRE administrator for the correct URL)\nClick on the ‘Browse’ link to see the full list of repositories that are in the Nexus mirror.\n\nIf you need to configure an application to use a nexus repo, you can click the ‘copy’ button in the URL column for the mirror that you are interested in. Then you can set your application to use this mirror.\nFor instance, if you want to configure python to use the pypi mirror on the Nexus server you could type:\npip config --user set global.index https://nexus-[TRE-ID].[LOCATION].cloudapp.azure.com/repository/pypi/pypi\npip config --user set global.index-url https://nexus-[TRE-ID].[LOCATION].cloudapp.azure.com/pypi/simple\npip config --user set global.trusted-host nexus-[TRE-ID].[LOCATION].cloudapp.azure.com\nFor Python, you may find that this has already been setup on your VM. You can check this by running:\npip config list\nSimilarly, on a Linux VM, the VM will be configured to use the apt repositories from Nexus. If you run sudo apt update, you will see that apt will only search for updates from the Nexus server."
  },
  {
    "objectID": "anatomy/aml.html",
    "href": "anatomy/aml.html",
    "title": "Azure Machine Learning",
    "section": "",
    "text": "See also\n\nAzure Anatomy FAQ\nAzure Machine Learning docs\n\nAzure Machine Learning (AML) is a cloud-based service within the Azure ecosystem that provides tools and services to build, train, and deploy machine learning models. It offers a collaborative environment for data scientists and developers, allowing them to leverage pre-built algorithms, manage data, and use compute resources efficiently. AML streamlines the end-to-end machine learning lifecycle, making it easier to develop, experiment, and operationalize AI solutions at scale."
  },
  {
    "objectID": "vignettes/data_lake_access.html",
    "href": "vignettes/data_lake_access.html",
    "title": "Accessing the EHR data from the TRE",
    "section": "",
    "text": "This notebook describes the process of accessing EHR data stored within a TRE-accessible Azure Data Lake Storage Gen2 (ADLS) instance.\nNote that PII has been masked from reports stored in the TRE and as a consequence structural changes may have appeared with respect to the original data.\nFeatures intended for use as inputs to machine learning models should not include or be derived from structural information from reports (e.g. line breaks, sentence length) stored in the storage instance detailed below.\n\n\nOpen the link shown in a browser outside of the TRE, enter the code and log in with your user account\n\n!az login --use-device-code\n\n\n\n\n\nstorage_account_name = \"stpixldflowehrprod\"\ninput_data_fs_name=\"data-lake-storage-pixld-flowehr-prod\"\ndata_directory_path=\"/\"\n\nImport dependencies and define functions to query data\n\n#Function definitions inspired by MS docs\n#at https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-directory-file-acl-python\nimport os, uuid, sys\nfrom azure.storage.filedatalake import DataLakeServiceClient\nfrom azure.core._match_conditions import MatchConditions\nfrom azure.storage.filedatalake._models import ContentSettings\nfrom azure.identity import DefaultAzureCredential, AzureCliCredential\n\nclass StorageClient:\n    def __init__(self, storage_account_name):\n        self.storage_account_name = storage_account_name\n        self.service_client = self.initialize_storage_account_ad()\n\n    def initialize_storage_account_ad(self):\n        try:          \n            credential = AzureCliCredential()\n            service_client = DataLakeServiceClient(account_url=f\"https://{self.storage_account_name}.dfs.core.windows.net\", credential=credential)\n\n            return service_client\n        except Exception as e:\n            print(e)\n            return\n\n    def download_file_from_directory(self, file_syst, directory, file_name):\n        try:\n            file_system_client = self.service_client.get_file_system_client(file_system=file_syst)\n            directory_client = file_system_client.get_directory_client(directory)\n            if not os.path.exists(\"downloaded_data\"):\n                os.makedirs(\"downloaded_data\")\n            print\n            local_file = open(f\"downloaded_data/{file_name}\",'wb')\n            file_client = directory_client.get_file_client(file_name)\n            download = file_client.download_file()\n            downloaded_bytes = download.readall()\n            local_file.write(downloaded_bytes)\n            local_file.close()\n            return\n        except Exception as e:\n            print(e)\n            return\n                          \n    def list_directory_contents(self, file_syst, directory):\n        try:\n            file_system_client = self.service_client.get_file_system_client(file_system=file_syst)\n\n            paths = file_system_client.get_paths(path=directory)\n\n            path_list = []\n            for path in paths:\n                path_list.append(path.name)\n            return path_list\n\n        except Exception as e:\n            print(e)\n            return\n                             \n    def create_directory(self, file_syst, directory):\n        try:\n            file_system_client = self.service_client.get_file_system_client(file_system=file_syst)\n            file_system_client.create_directory(directory)\n            return\n        except Exception as e:\n            print(e)\n            return\n    def upload_file_to_directory(self, file_syst, directory, uploaded_file_name, file_to_upload):\n        try:\n            file_system_client = self.service_client.get_file_system_client(file_system=file_syst)\n            directory_client = file_system_client.get_directory_client(directory)\n            file_client = directory_client.create_file(uploaded_file_name)\n            with open(file_to_upload, 'r') as local_file:\n                file_contents = local_file.read()\n                file_client.append_data(data=file_contents, offset=0, length=len(file_contents))\n                file_client.flush_data(len(file_contents))\n            return\n        except Exception as e:\n            print(e)\n            return\n\n\n\n\n\nclient = StorageClient(storage_account_name)\n\n\n\n\n\navailable_files = client.list_directory_contents(input_data_fs_name, data_directory_path)\nprint(available_files)\n\n\n\n\n\n[client.download_file_from_directory(input_data_fs_name, data_directory_path, datafile.rsplit(\"/\",1)[-1]) for datafile in available_files]\n\n\n\n\n\nclient.download_file_from_directory(input_data_fs_name, data_directory_path, available_files[0].rsplit(\"/\", 1)[-1])\n\n\n\n\n\nimport re\nimport pandas as pd\nparquet = []\ncsv = []\nfor x in available_files:\n    parquet_re = re.match(\"^.*\\.parquet\", x)\n    csv_re = re.match(\"^.*\\.csv\", x)\n    if parquet_re is not None:\n        parquet.append(parquet_re.group(0))\n    if csv_re is not None:\n        csv.append(csv_re.group(0))\n\n\n\n\nlocal_df = pd.read_parquet(f\"downloaded_data/{parquet[0].rsplit('/',1)[-1]}\")\nlocal_df.head()\n\n\n\n\n\nlocal_df = pd.read_csv(f\"downloaded_data/{csv[0].rsplit('/',1)[-1]}\")\nlocal_df.head()"
  },
  {
    "objectID": "vignettes/data_lake_access.html#reading-downloaded-files-with-pandas",
    "href": "vignettes/data_lake_access.html#reading-downloaded-files-with-pandas",
    "title": "Accessing the EHR data from the TRE",
    "section": "",
    "text": "import re\nimport pandas as pd\nparquet = []\ncsv = []\nfor x in available_files:\n    parquet_re = re.match(\"^.*\\.parquet\", x)\n    csv_re = re.match(\"^.*\\.csv\", x)\n    if parquet_re is not None:\n        parquet.append(parquet_re.group(0))\n    if csv_re is not None:\n        csv.append(csv_re.group(0))\n\n\n\n\nlocal_df = pd.read_parquet(f\"downloaded_data/{parquet[0].rsplit('/',1)[-1]}\")\nlocal_df.head()\n\n\n\n\n\nlocal_df = pd.read_csv(f\"downloaded_data/{csv[0].rsplit('/',1)[-1]}\")\nlocal_df.head()"
  },
  {
    "objectID": "vignettes/dicom_access.html",
    "href": "vignettes/dicom_access.html",
    "title": "Accessing the DICOM service from the TRE",
    "section": "",
    "text": "Note: DICOM data accessed through this service has been anonymised to some degree and will not mirror data from the original source.\n\nPII has been removed\nDates have been moved\n\n\nimport requests\nimport pydicom\nfrom pathlib import Path\nfrom urllib3.filepost import encode_multipart_formdata, choose_boundary\nfrom azure.identity import DefaultAzureCredential\n\n\n\n\nservice_url=\"https://hdspixldflowehrprod-dicom-pixld-flowehr-prod.dicom.azurehealthcareapis.com\"\nversion=\"v1\"\nbase_url = f\"{service_url}/{version}\"\nprint(service_url)\n\n\n\n\nEnter the provided code in a browser outside of the TRE VM\n\n!az login --use-device-code\n\nEnsure the correct subscription is set as the ‘default’ subscription. Please select the subscription name you would like to use for futher authentication against the DICOM service from the list of subscriptions returned by the previous cell.\nReplace your-subscription-name with the actual subscription name in the below cell and run the cell.\n\n!az account set --subscription \"your-subscription-name\"\n\n\n\n\n\nfrom azure.identity import DefaultAzureCredential, AzureCliCredential\ncredential = DefaultAzureCredential()\ntoken = credential.credentials[3].get_token('https://dicom.healthcareapis.azure.com')\nbearer_token = f'Bearer {token.token}'\n\n\n\n\nGenerates an equivalent token to the above cell, may be used if problems with DefaultAzureCredential are encountered.\n\ncredential = AzureCliCredential()\nbearer_token = f\"Bearer {credential.get_token('https://dicom.healthcareapis.azure.com').token}\"\n\n\n\n\n\nclient = requests.session()\n\n\n\n\n\nheaders = {\"Authorization\":bearer_token}\nurl= f'{base_url}/changefeed'\n\nresponse = client.get(url,headers=headers)\nif (response.status_code != 200):\n    print('Error! Likely not authenticated!')\nprint(response.status_code)\n\n\n\n\n\n\n\nurl = f\"{base_url}/studies\"\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nresponse_query = client.get(url, headers=headers)\nprint(f\"{response_query.status_code=}, {response_query.content=}\")\n\nExtract study IDs from response content - returned as bytes\nStudyInstanceUID corresponds to 0020000D - see the DICOM documentation for details\n\nimport json\nr = json.loads(response_query.content.decode())\nstudy_uids = [study[\"0020000D\"][\"Value\"][0] for study in r]\n\n\n\n\n\nstudy_uid = study_uids[0] # as an example, use the previous query\nurl = f\"{base_url}/studies\"\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nparams = {'StudyInstanceUID':study_uid}\nresponse_query = client.get(url, headers=headers, params=params)\nprint(f\"{response_query.status_code=}, {response_query.content=}\")\n\nReturn series UIDs within a single study\n\nurl = f'{base_url}/studies/{study_uids[0]}/series'\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nresponse = client.get(url, headers=headers)\nprint(f\"{response.status_code=}, {response.content=}\")\n\nExtract series IDs from response content - returned as bytes\nSeriesInstanceUID corresponds to 0020000E - see the DICOM documentation for details\n\nr = json.loads(response.content.decode())\nseries_uids = [study[\"0020000E\"][\"Value\"][0] for study in r]\n\nSearch within study by series UID\n\nseries_uid = series_uids[0]\nurl = f'{base_url}/studies/{study_uid}/series'\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nparams = {'SeriesInstanceUID':series_uid}\n\nresponse = client.get(url, headers=headers, params=params) #, verify=False)\nprint(f\"{response.status_code=}, {response.content=}\")\n\nSearch all studies by series UID\n\nurl = f'{base_url}/series'\nheaders = {'Accept': 'application/dicom+json', \"Authorization\":bearer_token}\nparams = {'SeriesInstanceUID': series_uid}\nresponse = client.get(url, headers=headers, params=params)\nprint(f\"{response.status_code=}, {response.content=}\")\n\n\n\n\n\n\n\n\nurl = f'{base_url}/studies/{study_uid}'\nheaders = {'Accept':'multipart/related; type=\"application/dicom\"; transfer-syntax=*', \"Authorization\":bearer_token}\n\nresponse = client.get(url, headers=headers)\n\nInstances are retrieved as bytes - to return useful output, we’ll loop through returned items and convert to files that can be read by pydicom\n\nimport requests_toolbelt as tb\nfrom io import BytesIO\n\nmpd = tb.MultipartDecoder.from_response(response)\n\nretrieved_dcm_files = []\nfor part in mpd.parts:\n    # headers returned as binary\n    print(part.headers[b'content-type'])\n    \n    dcm = pydicom.dcmread(BytesIO(part.content))\n    print(dcm.PatientName)\n    print(dcm.SOPInstanceUID)\n    retrieved_dcm_files.append(dcm)\n\n\nprint(retrieved_dcm_files[0].file_meta)\n\n\n\n\n\nresponse_array = []\nfor study_uid in study_uids:\n    url = f'{base_url}/studies/{study_uid}'\n    headers = {'Accept':'multipart/related; type=\"application/dicom\"; transfer-syntax=*', \"Authorization\":bearer_token}\n    response = client.get(url, headers=headers)\n    response_array.append(response)\n\nParse returned items and output a list of lists, with a list of instances per study\n\nimport requests_toolbelt as tb\nfrom io import BytesIO\n\nretrieved_dcm_files_multistudy = []\nfor r in response_array:\n    mpd = tb.MultipartDecoder.from_response(r)\n\n    retrieved_study_dcm_files = []\n    for part in mpd.parts:\n        dcm = pydicom.dcmread(BytesIO(part.content))\n        retrieved_study_dcm_files.append(dcm)\n    retrieved_dcm_files_multistudy.append(retrieved_study_dcm_files)\n\n\nprint(retrieved_dcm_files_multistudy[0][0].file_meta)\n\n\n\n\n\nInstance images can be viewed by plotting the pixel array with matplotlib (or a similar library)\n\nimport matplotlib.pyplot as plt\nplt.imshow(retrieved_dcm_files[0].pixel_array, cmap=plt.cm.bone)\n\n\n\n\nExtract instance IDs from response content - returned as bytes\nSOPInstanceUID corresponds to 00080018 - see the DICOM documentation for details\n\nstudy_uid, series_uid = study_uids[0], series_uids[0]\nurl = f'{base_url}/studies/{study_uid}/series/{series_uid}/instances'\nheaders = {'Accept': 'application/dicom+json', \"Authorization\":bearer_token}\nresponse = client.get(url, headers=headers)\n\nr = json.loads(response.content.decode())\ninstance_uids = [series[\"00080018\"][\"Value\"][0] for series in r]\n\n\ninstance_uid = instance_uids[0]\nurl = f'{base_url}/studies/{study_uid}/series/{series_uid}/instances/{instance_uid}'\nheaders = {'Accept':'application/dicom; transfer-syntax=*', \"Authorization\":bearer_token}\n\nresponse = client.get(url, headers=headers)\n\nAgain, the single instance is returned as bytes, which we can pass to pydicom with\n\ndicom_file = pydicom.dcmread(BytesIO(response.content))\nprint(dicom_file.PatientName)\nprint(dicom_file.SOPInstanceUID)\nprint(dicom_file.file_meta)\n\n\nplt.imshow(dicom_file.pixel_array, cmap=plt.cm.bone)"
  },
  {
    "objectID": "vignettes/dicom_access.html#create-a-requests-session",
    "href": "vignettes/dicom_access.html#create-a-requests-session",
    "title": "Accessing the DICOM service from the TRE",
    "section": "",
    "text": "client = requests.session()"
  },
  {
    "objectID": "vignettes/dicom_access.html#verify-authentication-has-performed-correctly",
    "href": "vignettes/dicom_access.html#verify-authentication-has-performed-correctly",
    "title": "Accessing the DICOM service from the TRE",
    "section": "",
    "text": "headers = {\"Authorization\":bearer_token}\nurl= f'{base_url}/changefeed'\n\nresponse = client.get(url,headers=headers)\nif (response.status_code != 200):\n    print('Error! Likely not authenticated!')\nprint(response.status_code)"
  },
  {
    "objectID": "vignettes/dicom_access.html#querying-the-dicom-service",
    "href": "vignettes/dicom_access.html#querying-the-dicom-service",
    "title": "Accessing the DICOM service from the TRE",
    "section": "",
    "text": "url = f\"{base_url}/studies\"\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nresponse_query = client.get(url, headers=headers)\nprint(f\"{response_query.status_code=}, {response_query.content=}\")\n\nExtract study IDs from response content - returned as bytes\nStudyInstanceUID corresponds to 0020000D - see the DICOM documentation for details\n\nimport json\nr = json.loads(response_query.content.decode())\nstudy_uids = [study[\"0020000D\"][\"Value\"][0] for study in r]\n\n\n\n\n\nstudy_uid = study_uids[0] # as an example, use the previous query\nurl = f\"{base_url}/studies\"\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nparams = {'StudyInstanceUID':study_uid}\nresponse_query = client.get(url, headers=headers, params=params)\nprint(f\"{response_query.status_code=}, {response_query.content=}\")\n\nReturn series UIDs within a single study\n\nurl = f'{base_url}/studies/{study_uids[0]}/series'\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nresponse = client.get(url, headers=headers)\nprint(f\"{response.status_code=}, {response.content=}\")\n\nExtract series IDs from response content - returned as bytes\nSeriesInstanceUID corresponds to 0020000E - see the DICOM documentation for details\n\nr = json.loads(response.content.decode())\nseries_uids = [study[\"0020000E\"][\"Value\"][0] for study in r]\n\nSearch within study by series UID\n\nseries_uid = series_uids[0]\nurl = f'{base_url}/studies/{study_uid}/series'\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nparams = {'SeriesInstanceUID':series_uid}\n\nresponse = client.get(url, headers=headers, params=params) #, verify=False)\nprint(f\"{response.status_code=}, {response.content=}\")\n\nSearch all studies by series UID\n\nurl = f'{base_url}/series'\nheaders = {'Accept': 'application/dicom+json', \"Authorization\":bearer_token}\nparams = {'SeriesInstanceUID': series_uid}\nresponse = client.get(url, headers=headers, params=params)\nprint(f\"{response.status_code=}, {response.content=}\")"
  },
  {
    "objectID": "vignettes/dicom_access.html#retrieve-all-instances-within-a-study",
    "href": "vignettes/dicom_access.html#retrieve-all-instances-within-a-study",
    "title": "Accessing the DICOM service from the TRE",
    "section": "",
    "text": "url = f'{base_url}/studies/{study_uid}'\nheaders = {'Accept':'multipart/related; type=\"application/dicom\"; transfer-syntax=*', \"Authorization\":bearer_token}\n\nresponse = client.get(url, headers=headers)\n\nInstances are retrieved as bytes - to return useful output, we’ll loop through returned items and convert to files that can be read by pydicom\n\nimport requests_toolbelt as tb\nfrom io import BytesIO\n\nmpd = tb.MultipartDecoder.from_response(response)\n\nretrieved_dcm_files = []\nfor part in mpd.parts:\n    # headers returned as binary\n    print(part.headers[b'content-type'])\n    \n    dcm = pydicom.dcmread(BytesIO(part.content))\n    print(dcm.PatientName)\n    print(dcm.SOPInstanceUID)\n    retrieved_dcm_files.append(dcm)\n\n\nprint(retrieved_dcm_files[0].file_meta)\n\n\n\n\n\nresponse_array = []\nfor study_uid in study_uids:\n    url = f'{base_url}/studies/{study_uid}'\n    headers = {'Accept':'multipart/related; type=\"application/dicom\"; transfer-syntax=*', \"Authorization\":bearer_token}\n    response = client.get(url, headers=headers)\n    response_array.append(response)\n\nParse returned items and output a list of lists, with a list of instances per study\n\nimport requests_toolbelt as tb\nfrom io import BytesIO\n\nretrieved_dcm_files_multistudy = []\nfor r in response_array:\n    mpd = tb.MultipartDecoder.from_response(r)\n\n    retrieved_study_dcm_files = []\n    for part in mpd.parts:\n        dcm = pydicom.dcmread(BytesIO(part.content))\n        retrieved_study_dcm_files.append(dcm)\n    retrieved_dcm_files_multistudy.append(retrieved_study_dcm_files)\n\n\nprint(retrieved_dcm_files_multistudy[0][0].file_meta)"
  },
  {
    "objectID": "vignettes/dicom_access.html#view-an-image-with-matplotlib",
    "href": "vignettes/dicom_access.html#view-an-image-with-matplotlib",
    "title": "Accessing the DICOM service from the TRE",
    "section": "",
    "text": "Instance images can be viewed by plotting the pixel array with matplotlib (or a similar library)\n\nimport matplotlib.pyplot as plt\nplt.imshow(retrieved_dcm_files[0].pixel_array, cmap=plt.cm.bone)"
  },
  {
    "objectID": "vignettes/dicom_access.html#retrieve-a-single-instance-within-a-study",
    "href": "vignettes/dicom_access.html#retrieve-a-single-instance-within-a-study",
    "title": "Accessing the DICOM service from the TRE",
    "section": "",
    "text": "Extract instance IDs from response content - returned as bytes\nSOPInstanceUID corresponds to 00080018 - see the DICOM documentation for details\n\nstudy_uid, series_uid = study_uids[0], series_uids[0]\nurl = f'{base_url}/studies/{study_uid}/series/{series_uid}/instances'\nheaders = {'Accept': 'application/dicom+json', \"Authorization\":bearer_token}\nresponse = client.get(url, headers=headers)\n\nr = json.loads(response.content.decode())\ninstance_uids = [series[\"00080018\"][\"Value\"][0] for series in r]\n\n\ninstance_uid = instance_uids[0]\nurl = f'{base_url}/studies/{study_uid}/series/{series_uid}/instances/{instance_uid}'\nheaders = {'Accept':'application/dicom; transfer-syntax=*', \"Authorization\":bearer_token}\n\nresponse = client.get(url, headers=headers)\n\nAgain, the single instance is returned as bytes, which we can pass to pydicom with\n\ndicom_file = pydicom.dcmread(BytesIO(response.content))\nprint(dicom_file.PatientName)\nprint(dicom_file.SOPInstanceUID)\nprint(dicom_file.file_meta)\n\n\nplt.imshow(dicom_file.pixel_array, cmap=plt.cm.bone)"
  },
  {
    "objectID": "quickstart/20_setup_gitea.html",
    "href": "quickstart/20_setup_gitea.html",
    "title": "Gitea set-up",
    "section": "",
    "text": "You will need a username and password for the TRE hosted Gitea instance. Now go to https://gitea-uclhtreprod.azurewebsites.net/ from a browser in the TRE.\n\nLog on\n\nYou’ll also want to run\ngit config --global user.name\ngit config --global user.email \nwith your Github identity, so that you’re commits are written with your real github handle\n\nAnd to save you having to re-enter your username and password on every interaction with Gitea then\ngit config --global credential.helper store\nvia SO"
  },
  {
    "objectID": "quickstart/60_deployment.html",
    "href": "quickstart/60_deployment.html",
    "title": "Author & deploy a FlowEHR App",
    "section": "",
    "text": "Let’s deploy your first FlowEHR App!"
  },
  {
    "objectID": "quickstart/60_deployment.html#pick-a-seedling",
    "href": "quickstart/60_deployment.html#pick-a-seedling",
    "title": "Author & deploy a FlowEHR App",
    "section": "1 Pick a 🌱 Seedling",
    "text": "1 Pick a 🌱 Seedling\nThe first step is to pick a seedling. A seedling is a pre-built FlowEHR App template that you can use as a starting point for your own app. You can find a list of seedlings in the UCLH Foundry GitHub Org. Which you use largely depends on which frameworks you want, whether you’re building a dashboard, web API, model etc.\nIf you have no strong preference at this point, the Dash Seedling is a good place to start for building a simple dashboard."
  },
  {
    "objectID": "quickstart/60_deployment.html#add-app-to-flowehr-config",
    "href": "quickstart/60_deployment.html#add-app-to-flowehr-config",
    "title": "Author & deploy a FlowEHR App",
    "section": "2 Add app to FlowEHR config",
    "text": "2 Add app to FlowEHR config\nNext we need to configure FlowEHR and tell it to plant our app seedling. Create your config in the FlowEHR repository, in the /apps folder, as follows:\n\nLocallyCI/CD\n\n\nCopy the apps.sample.yaml to a new apps.local.yaml file:\ncp apps.sample.yaml apps.local.yaml\n\n\nCopy the apps.sample.yaml to a new apps.{ENVIRONMENT}.yaml file, where {ENVIRONMENT} is the name of the FlowEHR environment you’re deploying via CI/CD (i.e. dev):\ncp apps.sample.yaml apps.{ENVIRONMENT}.yaml\n\n\n\nThen, amend any relevant config values or keep the defaults. Three things you might want to change are:\n\nThe key (dash_seedling in the sample file): this is the unique identifier for your app and will be used to name the repository and other infrastructure deployed for hosting the app.\n\n\n\n\n\n\n\nNote\n\n\n\nEach app listed in your config must have a unique key, and that key cannot contain spaces or special characters except ‘-’ and ’_’.\n\n\n\nowners / contributors: maps in the format of GH_USERNAME: AD_EMAIL. These record who should be added to the GitHub repo that’s created for your app. Owners get a maintainer role on the repo and contributors get member, and both roles are provided local access to sythetic feature store and state store data via their AD emails. For testing, you should add at least yourself as an owner.\nIf you decided on a different Seedling template to the Dash Seedling earlier, replace the managed_repo: template value with the Seedling you chose.\n\nYou can view more information on all the configuration options in the administrator’s configuration guide, and more information on GitHub roles here."
  },
  {
    "objectID": "quickstart/60_deployment.html#deploy-your-app-infrastructure",
    "href": "quickstart/60_deployment.html#deploy-your-app-infrastructure",
    "title": "Author & deploy a FlowEHR App",
    "section": "3 Deploy your app infrastructure",
    "text": "3 Deploy your app infrastructure\nAfter you’ve configured your app, you can run make apps from the root of the FlowEHR repository:\nmake apps\nThis will deploy the Azure infrastructure and GitHub artifacts for the configured app(s). This includes:\n\nA GitHub repository for your app\nA GitHub deployment branch matching your FlowEHR environment\nAn App Service for hosting your app\nA Cosmos DB database for storing your app’s state data\nA managed connection between the app hosting and the SQL Feature Store\n\nOnce the deployment has completed, you can find your newly-created repository in the GitHub organization you specified, with the repo name matching the App Id you specified in your apps config.\n\nYou should also see an extra branch in addition to the default (‘main’) with the name of your FlowEHR environment (e.g. in the below example, ‘infra-test’):"
  },
  {
    "objectID": "quickstart/60_deployment.html#deploy-your-app-code",
    "href": "quickstart/60_deployment.html#deploy-your-app-code",
    "title": "Author & deploy a FlowEHR App",
    "section": "4 Deploy your app code",
    "text": "4 Deploy your app code\nNow all the infrastructure has been set up for our app, we can deploy it to hosting. First, clone the repository locally:\ngit clone {YOUR_REPO_URL}\n\n4.1 Modify locally\nCreate a new branch for a quick superficial change to the code:\ngit branch -b initial-deploy\nGo and make a small change to the code, e.g. for the Dash template, change the seed name in app/app.py from hibiscus to magnolia. Commit your changes and push to GitHub.\ngit add .\ngit commit -m \"Initial deploy\"\ngit push\n\n\n4.2 Create a pull request\nNow, go to the GitHub repository and create a pull request from your new branch to the deployment branch (e.g. initial-deploy to infra-test). Then, embrace your inner anarchist and merge the pull request without review (as you’re an owner you should have permissions to do this via the generated CODEOWNERS file).\n\n\n\n\n\n\n\nImportant\n\n\n\nIt goes without saying; this is not how you should be deploying your apps when operating FlowEHR properly! This is just a quick way to get your app deployed for the purposes of this quickstart. In normal operation, all merges should require approvals and you should consider enforcing more than 1 review (via the apps config). Also, you should consider first PRing to main, then batching PRs to the deployment branch, ensuring that you have a good test suite in place to catch any issues.\n\n\n\n\n4.3 Check your app is deployed\nOn successful merge to the deployment branch, this will kick off a GitHub Actions workflow that will automatically build your app cointainer and deploy it to your FlowEHR environment, using secrets that were generated as part of make apps. You can check the status of this workflow in the Actions tab of your repository. The workflow will be called Deploy {ENVIRONMENT}.\n\nIf this has succeeded, you should be able to navigate to the Azure portal, select App Services, and find the app service that was deployed for your app. It will be named like so: webapp-{app_id}-{flowehr_suffix}.\nWhen you’ve found it, click on it and select Browse. You should be prompted to sign into Active Directory, then you should see your app!"
  },
  {
    "objectID": "quickstart/60_deployment.html#next-steps",
    "href": "quickstart/60_deployment.html#next-steps",
    "title": "Author & deploy a FlowEHR App",
    "section": "5 Next steps",
    "text": "5 Next steps\nCongratulations! You’ve deployed your first app to FlowEHR. You can find more information on building apps, accessing data and using the state store by visiting the developer’s guide."
  },
  {
    "objectID": "quickstart/20_setup_conda.html",
    "href": "quickstart/20_setup_conda.html",
    "title": "Setting up conda",
    "section": "",
    "text": "You need to use the pre-configured conda channels, which means that a environment.yaml file with the following will not work.\nname: los_predictor_v1\nchannels:\n  - defaults  # &lt;--- default channel not available\ndependencies:\n  - python=3.10.10\n  - pandas\n    pip:\n    - matplotlib\nSo comment out the line channels section!\nOpen a terminal window\nconda env create -f environment.yaml\n\n\n0.1 Notes\n\nConda has been configured to use a local mirror of conda and conda-forge in a framework called Nexus. This means that you will have access to most, but not all, packages and some of them will be outdated.\nThere are some conda environments that have already been made with pytorch/tensorflow/automl. Ideally those should be used in the first instance and packages can be added to that.\nIf you wish to create your own environment, ensure that you have activated your base conda environment using conda activate prior to creating any other conda environments\nIf you do wish to install other conda-like installers (e.g. mamba), ensure that the base conda environment has been activated so that the configuration files for Nexus has migrated into the mamba configuration.\n\nYou’ll probably have more success with conda-forge as your default channel as Nexus mirrors that more successfully. To do this, run: conda config --add channels conda-forge\n\nOn first use, jump into a terminal and conda init"
  },
  {
    "objectID": "quickstart/30_connecting_feature_store.html",
    "href": "quickstart/30_connecting_feature_store.html",
    "title": "Connecting to the Feature Store",
    "section": "",
    "text": "Within the TRE, use the az command line tools: The following will create an interactive login workflow in the browser in the TRE.\nThis might (the first time) provide you with a short alphanumeric token on the command line (e.g. ABC123), and then ask you to authenticate externally (outside the TRE) at https://microsoft.com/devicelogin. Here you enter the token, and then follow the standard 2FA authentication workflow using your UCLH SDE credentials (e.g. NHS email address, NHS email password, and the 2FA).\nSubsequently, the following should suffice\nAnd you can create tokens etc. as per\nWorking from a Python script, then the following will avoid the interactivity needed above. First you’ll need to be able to grab the token.\nUsing pyodbc\nUsing SQLAlchemy\nPutting it all together,"
  },
  {
    "objectID": "quickstart/30_connecting_feature_store.html#notes",
    "href": "quickstart/30_connecting_feature_store.html#notes",
    "title": "Connecting to the Feature Store",
    "section": "1 Notes",
    "text": "1 Notes\n\n1.1 Authenticate via the Azure CLI\nThe docs provided for the Azure-Identity package are helpful:\n\nDefaultAzureCredential and AzureCliCredential can authenticate as the user signed in to the Azure CLI. To sign in to the Azure CLI, run az login. On a system with a default web browser, the Azure CLI will launch the browser to authenticate a user.\n\n\nWhen no default browser is available, az login will use the device code authentication flow. This can also be selected manually by running az login –use-device-code."
  },
  {
    "objectID": "quickstart/20_setup_overview.html",
    "href": "quickstart/20_setup_overview.html",
    "title": "Set-up overview",
    "section": "",
    "text": "You are likely to need the following tools and services"
  },
  {
    "objectID": "quickstart/20_setup_overview.html#tools",
    "href": "quickstart/20_setup_overview.html#tools",
    "title": "Set-up overview",
    "section": "1 Tools",
    "text": "1 Tools\n\nAzure Data Studio\nVS Code\nConda"
  },
  {
    "objectID": "quickstart/20_setup_overview.html#services",
    "href": "quickstart/20_setup_overview.html#services",
    "title": "Set-up overview",
    "section": "2 Services",
    "text": "2 Services\n\nVirtual Machine\nGitea\nAzure Machine Learning\nFlowEHR feature store"
  },
  {
    "objectID": "quickstart/20_setup_overview.html#how-to-set-up-flowehr-on-azure",
    "href": "quickstart/20_setup_overview.html#how-to-set-up-flowehr-on-azure",
    "title": "Set-up overview",
    "section": "3 How to set-up FlowEHR on Azure",
    "text": "3 How to set-up FlowEHR on Azure\nSee the onboarding for the data scientist docs for more details.\n\nYou will be working from your laptop, and interacting through a browser (e.g. Edge, Chrome etc.)\nAsk your FlowEHR administrator for access to the Azure TRE landing zone. They will provide you with a link to the landing page, and add your NHS.net credentials to the list of approved users.\n\nLogin using your NHS.net email. You will need to have 2-factor authentication enabled\nSet-up a virtual machine from one of the provided templates. Details in the Azure TRE docs\n\nInbound and outbound traffic to the TRE is strictly controlled so you will find the following services to enable you to work\n\na mirror of Python (via Conda, and PyPi), R and other package managers provided by Nexus\nan internal Git server provided by Gitea\nan airlock for moving items into and out from the TRE. Local polices will enforce controls to protect data, and the environment.\n\n\nYou should find yourself with a suite of open source tools designed for modern data science tasks:\n\nJupyter\nRStudio\nIntegrated Development Environments (e.g. PyCharm) and text editors (e.g. VS Code)\nA database GUI\n\nSee also\n\nTrusted Research Environments\nVirtual Machines and in particular\n\nRunning Jupyter"
  },
  {
    "objectID": "quickstart/70_stewardship.html",
    "href": "quickstart/70_stewardship.html",
    "title": "Stewardship",
    "section": "",
    "text": "Models should not be deployed without a monitoring strategy. To this end, we provide a template that addresses data drift and model drift. These steps are in addition to the data testing that is built into the feature engineering pipeline."
  },
  {
    "objectID": "quickstart/70_stewardship.html#key-terms",
    "href": "quickstart/70_stewardship.html#key-terms",
    "title": "Stewardship",
    "section": "1 Key terms",
    "text": "1 Key terms\n\nGround Truth: the true answer\nPredicted or Inferred value or label: the model or the machines best guess\nTraining data: a sufficient sample of the training data set used compare against the new (unseen) data\nUnseen data: data that has arrived after the model has been trained and deployed. The data in production."
  },
  {
    "objectID": "quickstart/70_stewardship.html#walk-through",
    "href": "quickstart/70_stewardship.html#walk-through",
    "title": "Stewardship",
    "section": "2 Walk through",
    "text": "2 Walk through\n\nClone the ‘model seedling’ template"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FlowEHR Documentation",
    "section": "",
    "text": "FlowEHR is a development and deployment platform for digital research and innovation for healthcare providers. It provides technical controls to protect privacy but an smooth pathway for deployment of data driven products to the clinic or the bedside. Whilst it is a platform to bridge the AI chasm for health, we anticipate health data researchers, application developers, and clinical analysts to accompany us on our journey.\nOur aim is …\nThese are principles are sometimes framed as barries but only by full engaging with each of them will it be possible to use data to improve the quality and safety of health care. When combined we believe this will enable the deployment of digital tools to solve operational and clinical problems.\nOur original use case was the safe & effective development & deployment of real-time translational Machine Learning interventions into clinical and operational settings inside a busy NHS hospital."
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "FlowEHR Documentation",
    "section": "1 Next steps",
    "text": "1 Next steps\nWe would recommend using the QuickStart guide to follow and ‘end-to-end’ walkthrough of an application from concept to implementation. This will begin with deploying the FlowEHR infrastructure for personal use with synthetic data, and end with inspecting the performance of an algorithm in the (simulated) wild.\nSpecific users may wish to dive directly into documentation relevant to their role. This includes\n\nthe data scientist for those providing analytical insights or building models against live data\nthe application developer for those building and deploying applications\nthe platform maintainer for those responsible for installing and mananaging the platform\nthe algorithm steward for those using the platform to monitor, audit and observe data driven applications in use"
  },
  {
    "objectID": "faq/azure-anatomy.html",
    "href": "faq/azure-anatomy.html",
    "title": "What’s the difference between Azure, a landing zone, a TRE, and a workspace? And do I need to care?",
    "section": "",
    "text": "Hopefully you don’t need to care but these terms come up so it’s helpful to know what they mean. The list below starts from the general and moves to the specific.\n\nAzure\n\nAzure is a cloud computing platform and service offered by Microsoft that provides a range of cloud services, including computing, storage, networking, and analytics. It enables businesses and individuals to build, deploy, and manage applications and services through Microsoft-managed data centers.\n\nTrusted Research Environment (TRE) hosted on Azure\n\nA TRE hosted on Azure is a secure, privacy-preserving environment that allows researchers to access and analyze sensitive data without compromising the data’s confidentiality. It is built on Azure infrastructure, leveraging its security, compliance, and scalability features, to create a controlled environment for data access, analysis, and sharing.\n\nWorkspace within a TRE\n\nA workspace within a TRE is a dedicated, isolated area where researchers can access, store, and analyze data securely. It provides a collaborative space with controlled access and tools needed for data processing, analysis, and visualization while ensuring that the data remains protected and compliant with privacy regulations.\n\n\nAzure Machine Learning (AML): AML is a cloud-based service within the Azure ecosystem that provides tools and services to build, train, and deploy machine learning models. It offers a collaborative environment for data scientists and developers, allowing them to leverage pre-built algorithms, manage data, and use compute resources efficiently. AML streamlines the end-to-end machine learning lifecycle, making it easier to develop, experiment, and operationalize AI solutions at scale."
  },
  {
    "objectID": "faq/faq.html",
    "href": "faq/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Please browse our FAQs, and short ‘today I learned’ posts.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nWhat’s the difference between Azure, a landing zone, a TRE, and a workspace? And do I need to care?\n\n\n\n\n\n\n\nHow do I contribute to the documentation\n\n\nSteve Harris\n\n\n\n\nWhat is CosmosDB?\n\n\n\n\n\n\n\nWhat is EMAP?\n\n\n\n\n\n\n\nPlease summarise the approach to Information Security?\n\n\n\n\n\n\n\nRunning a Jupyter notebook from this documentation within a virtual environment\n\n\n\n\n\n\n\nWhat is algorithm stewardship?\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "faq/contributing.html",
    "href": "faq/contributing.html",
    "title": "How do I contribute to the documentation",
    "section": "",
    "text": "We welcome new contributions, edits to existing material, and ad hoc supporting material. The latter category could include example notebooks or vignettes showing how something works, or a quick note.\n\nNotebooks and vignettes can just be saved in ./docs/notebooks/. Please ensure the first cell in the notebook contains the YAML metadata for the author, title, and categories.\nQuick notes in the style FAQ answers, or ‘today I learned’ notes can be saved into ./docs/faq/\n\nBoth will be automatically indexed when the site is deployed. The documentation is deployed using Quarto, and the documentation is built from the ./docs/ directory. Please fork or clone .\nYou will need to have\n\ninstalled basic python and jupyter extensions necessary including those for jupyter\ninstalled quarto\ninstalled the VSCode extension\n\nWorkflow\ncd docs\nquarto preview\nThen create your content and review before making a pull request. Don’t forget that changes to config (e.g. _quarto.yml may need you to rerun quarto render to ensure the whole site is correctly rebuilt) When you push to either dev or main branches, then a GitHub Action (see .github/workflows/publish.yml) will be triggered that should publish the current documentation to https://hylode.github.io/HyUi/."
  },
  {
    "objectID": "faq/jupyter.html",
    "href": "faq/jupyter.html",
    "title": "Running a Jupyter notebook from this documentation within a virtual environment",
    "section": "",
    "text": "Within a TRE VM:\n\nVisit the Gitea URL provided alongside this documentation\nSelect ‘Explore’ from the top left\nSelect the ‘End-User-Docs’ repository\nCopy the HTTPS link to the repository\n\nThe following code\n\nClones this documentation repo\nCreates a virtual environment\nActivates the environment\nInstalls jupyterlab and ipykernel within the virtual environment\n\nMakes the virtual environment available as a kernel within a Jupyter Lab environment\nLaunches Jupyter Lab\n\nOpen a terminal within the VM and create a virtual environment with Conda\nconda create -n my_virtual_environment python=3.10\nconda activate my_virtual_environment\nYou may be prompted by conda to initialise your shell. Run\nconda init bash\nand restart your terminal. Run the following commands in the restarted terminal to clone this documentation repo and install dependencies\ngit clone ${gitea-repository-url}\ncd End-User-Docs/docs/notebooks\nconda create -n my_virtual_environment python=3.10\nconda activate my_virtual_environment\nconda install jupyterlab ipykernel\npip install -r requirements.txt\npython -m ipykernel install --user --name=my_env\nA Jupyter Lab environment can then be launched by running\njupyter lab\nWithin this environment, navigate to the example notebooks provided. When running the notebooks, ensure the python kernel is set to the name defined by the earlier python -m ipykernel install command."
  },
  {
    "objectID": "personas/administrator/administrator.html",
    "href": "personas/administrator/administrator.html",
    "title": "FlowEHR Garden Path",
    "section": "",
    "text": "You are will deploy, maintain and manage the FlowEHR platform for your healthcare provider."
  },
  {
    "objectID": "personas/steward/steward.html",
    "href": "personas/steward/steward.html",
    "title": "FlowEHR Garden Path",
    "section": "",
    "text": "You will manage the applications and models on the FlowEHR platform to ensure their quality and safety."
  },
  {
    "objectID": "personas/developer/developer.html",
    "href": "personas/developer/developer.html",
    "title": "FlowEHR Garden Path",
    "section": "",
    "text": "You will developer applications for healthcare, sometimes using models built by the data science team."
  },
  {
    "objectID": "personas/datascientist/datascientist.html",
    "href": "personas/datascientist/datascientist.html",
    "title": "FlowEHR Garden Path",
    "section": "",
    "text": "You will wrangle data, and build models.\nYou might come from a background of - NHS analytics - Data science - Machine learning for health"
  },
  {
    "objectID": "notes/walkthrough_v1.html",
    "href": "notes/walkthrough_v1.html",
    "title": "Walkthrough notes",
    "section": "",
    "text": "Scratch pad for planning the demo"
  },
  {
    "objectID": "personas/datascientist/onboarding.html",
    "href": "personas/datascientist/onboarding.html",
    "title": "Data science onboarding",
    "section": "",
    "text": "Use authenticator app - can be any authenticator app, google or microsoft\nRequires to have an NHS.net email. If you do not have access this needs to be arranged with an UCLH/Azure TRE Administrator\nView the available workspaces\n\nThis is a shared area commonly for people to collaborate on a project together.\nThis has shared storage"
  },
  {
    "objectID": "personas/datascientist/onboarding.html#login",
    "href": "personas/datascientist/onboarding.html#login",
    "title": "Data science onboarding",
    "section": "",
    "text": "Use authenticator app - can be any authenticator app, google or microsoft\nRequires to have an NHS.net email. If you do not have access this needs to be arranged with an UCLH/Azure TRE Administrator\nView the available workspaces\n\nThis is a shared area commonly for people to collaborate on a project together.\nThis has shared storage"
  },
  {
    "objectID": "personas/datascientist/onboarding.html#create-machine",
    "href": "personas/datascientist/onboarding.html#create-machine",
    "title": "Data science onboarding",
    "section": "2 Create machine",
    "text": "2 Create machine\n\nClick virtual machines (VM) to go into the personal list of available machines. Do not click on Connect as this will take you to a different place.\nCreate a VM with the specified ID - this can be anything of your choosing as this is only visible to you.\nUse the machine with the lowest resources (CPU and RAM) that facilitates your work. This can always be upgraded later if you need. We recommend starting at CPU=2, RAM=8GB\nWait for your VM to load, this might take a couple of minutes. You might need to refresh your screen to ensure that the VM has been deployed/provisioned/available."
  },
  {
    "objectID": "personas/datascientist/onboarding.html#python",
    "href": "personas/datascientist/onboarding.html#python",
    "title": "Data science onboarding",
    "section": "3 Python",
    "text": "3 Python\n\n3.1 Conda Environments\n\nConda has been configured to use a local mirror of conda and conda-forge in a framework called Nexus. This means that you will have access to most, but not all, packages and some of them will be outdated.\nThere are some conda environments that have already been made with pytorch/tensorflow/automl. Ideally those should be used in the first instance and packages can be added to that.\nIf you wish to create your own environment, ensure that you have activated your base conda environment using conda activate prior to creating any other conda environments\nIf you do wish to install other conda-like installers (e.g. mamba), ensure that the base conda environment has been activated so that the configuration files for Nexus has migrated into the mamba configuration.\n\nYou’ll probably have more success with conda-forge as your default channel as Nexus mirrors that more successfully. To do this, run: conda config --add channels conda-forge\n\nOn first use, jump into a terminal and conda init\n\n\n\n3.2 Pip Environments\n\nPip environments have also been configured to work with Nexus.\nThis means that any pip install command should work out of the box\nWe advise that you create a conda environment (as above), and install any packages that are not in conda using pip within that conda environment"
  },
  {
    "objectID": "personas/datascientist/onboarding.html#rrstudio",
    "href": "personas/datascientist/onboarding.html#rrstudio",
    "title": "Data science onboarding",
    "section": "4 R/RStudio",
    "text": "4 R/RStudio\n\nThere is an installation of R (version 4.1) and RStudio with basic packages.\nA mirror of packages available on CRAN has been developed and thus the installation of packages is possible:\n\nSpecifically, we’ve tested rstan and brms installations as they require downloading and compilation capabilities."
  },
  {
    "objectID": "personas/datascientist/onboarding.html#airlock",
    "href": "personas/datascientist/onboarding.html#airlock",
    "title": "Data science onboarding",
    "section": "5 Airlock",
    "text": "5 Airlock\nTo transfer files into the workspace/VM - there are two things you’ll need: 1. An airlock transfer request which gives you a link to a Storage Blob that is a temporary container link (SAS) to upload to 2. Microsoft Azure Storage Explorer app - this is available for Windows, MacOS, and Linux (as a snap and a tar)\n\n5.1 Import File Transfer\n\nEnsure that the file you wish to transfer is a single file - if you wish to transfer many files, zip them.\nIf you have not downloaded the Microsoft Azure Storage Explorer installed, do so now.\nClick on Airlock on the menu in the Azure TRE\nYou’ll be greeted with a list of current active requests (if any).\nTo start the process, click on New Request and then click on import - this will open a dialog on the right where you can name and describe your airlock transfer request\nOnce you’ve created the transfer request, you’ll be greeted with another dialog on the right describing the request. Copy the SAS URL - this step is important\nOpen the Microsoft Azure Storage Explorer. You’ll be greeted with a Get Started Page called Storage Explorer. If this is not available, exit and open it again or open a new window. Failing that, click on Help, then Reset\nClick on Attach to a resource - this will open a new window titled Select a Resource\nClick on Blob container\nThen select Shared access signature URL (SAS), then click Next\nThen past the SAS URL that copied from the Airlock transfer window in the Azure TRE into the blob container SAS URL. The Display name will be populated automatically.\nClick Next, and then Connect. After a moment, you will be greeted with a new Tab which has a Upload button on the top left hand side. Click that and select Upload Files...\nThis will open another dialog box titled Upload Files. Click on the ... next to the “No Files Selected”\nClick Upload. This will take a little while depending on the size of the file and the speed of your connection.\nGo back to the Azure TRE, you should still have the dialog with the Airlock open on the right hand size. If you have closed it, you can double click on the title of your upload that you created. Click on Submit.\nThis will now require a person to approve the ingress of the data into the Azure TRE. You can view the progress of this from this screen.\nOnce approved, your files will be located in the workspace"
  },
  {
    "objectID": "personas/datascientist/onboarding.html#integrated-development-environments-ides",
    "href": "personas/datascientist/onboarding.html#integrated-development-environments-ides",
    "title": "Data science onboarding",
    "section": "6 Integrated Development Environments (IDEs)",
    "text": "6 Integrated Development Environments (IDEs)\n\n6.1 Visual Studio Code\nThere is an installation of Visual Studio Code but has limited functionality for extensions due to security reasons. It also does not have access to Remote Extensions and thus docker installations within Remote Extensions is not currently possible.\n\n\n6.2 PyCharm Community Edition\nThere is an installation of PyCharm, it is the community edition and is thus limited in scope and it also does not have support for external extensions for the security reasons."
  },
  {
    "objectID": "personas/datascientist/onboarding.html#assorted-notes-and-tips",
    "href": "personas/datascientist/onboarding.html#assorted-notes-and-tips",
    "title": "Data science onboarding",
    "section": "7 Assorted notes and tips",
    "text": "7 Assorted notes and tips\n\nshared file system at /vm-shared-storage/\nwhen logging in to Azure ML within the TRE then you need to use the MFA token from https://portal.azure.com/#home not from NHS email"
  },
  {
    "objectID": "personas/developer/building-an-app.html",
    "href": "personas/developer/building-an-app.html",
    "title": "Building a FlowEHR App",
    "section": "",
    "text": "If you want to build an app that works safely with live patient data and/or ML Model predictions, share it securely with clinicians to inform their decision-making, and you don’t want to worry about managing any infrastructure and deployment logic, then you’re in the right place."
  },
  {
    "objectID": "personas/developer/building-an-app.html#getting-started",
    "href": "personas/developer/building-an-app.html#getting-started",
    "title": "Building a FlowEHR App",
    "section": "1 Getting started",
    "text": "1 Getting started\n\n1.1 Prerequisites\nSpeak to your friendly FlowEHR admin…\n\n\n1.2 Choosing a template\nFinding a seedling Cloning the repo and making some changes​ Serve-local to run + debug locally​"
  },
  {
    "objectID": "personas/developer/building-an-app.html#connecting-to-synthetic-data",
    "href": "personas/developer/building-an-app.html#connecting-to-synthetic-data",
    "title": "Building a FlowEHR App",
    "section": "2 Connecting to synthetic data",
    "text": "2 Connecting to synthetic data\n\n2.1 Local connectivity to Feature Store\n\n\n2.2 Local connectivity to State Store"
  },
  {
    "objectID": "personas/developer/building-an-app.html#calling-model-endpoints",
    "href": "personas/developer/building-an-app.html#calling-model-endpoints",
    "title": "Building a FlowEHR App",
    "section": "3 Calling model endpoints",
    "text": "3 Calling model endpoints"
  },
  {
    "objectID": "personas/developer/building-an-app.html#app-monitoring-instrumentation",
    "href": "personas/developer/building-an-app.html#app-monitoring-instrumentation",
    "title": "Building a FlowEHR App",
    "section": "4 App monitoring instrumentation",
    "text": "4 App monitoring instrumentation"
  },
  {
    "objectID": "personas/developer/building-an-app.html#deploying-your-app-to-a-dev-environment",
    "href": "personas/developer/building-an-app.html#deploying-your-app-to-a-dev-environment",
    "title": "Building a FlowEHR App",
    "section": "5 Deploying your app to a dev environment",
    "text": "5 Deploying your app to a dev environment\n\n5.1 PR to deployment branch\nPR to app-dev branch. My work is deployed in dev.​ PR to staging / approval – my dashboard now runs in a prod environment with access to live data. I can see it [here].​\n\n\n5.2 Viewing the hosted site\n\n\n5.3 Viewing logs and metrics"
  },
  {
    "objectID": "personas/developer/building-an-app.html#deploying-to-staging-prod-environments",
    "href": "personas/developer/building-an-app.html#deploying-to-staging-prod-environments",
    "title": "Building a FlowEHR App",
    "section": "6 Deploying to staging & prod environments",
    "text": "6 Deploying to staging & prod environments\nNow I’m ready + others approve =&gt; It can go to prod here.​ I can share the link, and a clinician can securely log in via AAD"
  },
  {
    "objectID": "personas/personas.html",
    "href": "personas/personas.html",
    "title": "Personas",
    "section": "",
    "text": "Infrastructure Engineer/Administrator: I create Azure infrastructure via Terraform code, and I am also known as the platform administrator or maintainer.\nData Scientist: I writes data transformation code, and build models. I could also be an NHS analyst where the task is similar but I am likely to use different tools (SSMS, PowerBI etc. and rely less on inference).\nInternal Innovator/Developer:\n\nApp Developer: I use a synthetic data store to build apps + dashboards.\nData Scientist: I use a TRE to build ML models against patient data.\n\nSteward: I monitors and manages ML models and apps. I have a background that is a convergence of the Clinical Safety Officer role, and an ML-Ops practitioner."
  },
  {
    "objectID": "personas/administrator/configuration.html#configuring-flowehr-apps",
    "href": "personas/administrator/configuration.html#configuring-flowehr-apps",
    "title": "Configuring FlowEHR",
    "section": "2 Configuring FlowEHR Apps",
    "text": "2 Configuring FlowEHR Apps\nSimilar to the behaviour of the root FlowEHR config, if the $ENVIRONMENT env var is set (typically by CI), FlowEHR will look for the apps configured in a file in the /apps directory called apps.{ENVIRONMENT}.yaml. If unset, it will look for apps.local.yaml.\nThese config files consist of a map of app_id and the config values for that app. It will also look for a matching app_id in the apps.yaml shared config file, and will merge properties of the two, with environment-specific properties taking precedence. This means you can define common values of an app (like the owners and contributors) that are common across environments, and only override relevant settings per environment (i.e. num_of_approvers).\n\n\n\n\n\n\nImportant\n\n\n\nFlowEHR will only deploy apps defined in the environment file matching the currently selected environment. If a file doesn’t exist for your current environment (i.e. if you’re working locally and don’t have an apps.local.yaml) or is empty, even if there are apps configured in the shared apps.yaml, FlowEHR will treat this as there being no apps to deploy. This ensures no apps are deployed accidentally to environment they shouldn’t be without it being explicitly set for that environment."
  },
  {
    "objectID": "faq/emap.html",
    "href": "faq/emap.html",
    "title": "What is EMAP?",
    "section": "",
    "text": "See https://github.com/inform-health-informatics/emap_documentation and in particular the technical overview"
  },
  {
    "objectID": "faq/information-security.html",
    "href": "faq/information-security.html",
    "title": "Please summarise the approach to Information Security?",
    "section": "",
    "text": "We benchmark ourselves against Center for Internet Security (CIS) Benchmarks\n\nThe Center for Internet Security (CIS) Benchmarks are a set of best practices and configuration guidelines developed to help organizations improve the security of their systems and networks. These benchmarks are developed through a collaborative process involving cybersecurity experts, industry professionals, and government representatives. They provide a comprehensive approach to hardening systems and reducing vulnerabilities by addressing various aspects of security, including configuration, user access control, and patch management.\nThe CIS Benchmarks cover a wide range of technologies, such as operating systems, network devices, cloud environments, and applications. Some examples include:\n\nMicrosoft Windows\nLinux distributions (e.g., Red Hat, Ubuntu, CentOS)\nmacOS\nNetwork devices (e.g., Cisco, Juniper, Fortinet)\nCloud platforms (e.g., Amazon Web Services, Microsoft Azure, Google Cloud Platform)\nDatabases (e.g., Oracle, Microsoft SQL Server, MySQL)\nWeb servers (e.g., Apache, Nginx, Microsoft IIS)\nVirtualization platforms (e.g., VMware, Microsoft Hyper-V)\n\nCIS Benchmarks help organizations by:\n\nEstablishing a baseline for security configuration: The benchmarks provide a clear starting point for organizations to assess their current security posture and make improvements as needed.\nFacilitating compliance: Many regulatory standards, such as the Payment Card Industry Data Security Standard (PCI DSS), the Health Insurance Portability and Accountability Act (HIPAA), and the General Data Protection Regulation (GDPR), require organizations to implement secure configurations. The CIS Benchmarks can help organizations meet these requirements.\nEnhancing system performance: By following the benchmarks, organizations can optimize their systems and reduce potential performance issues caused by misconfigurations.\nReducing attack surfaces: Properly securing systems can help organizations minimize the risk of data breaches and other cyber threats.\nCIS Benchmarks are available for free, and organizations can choose to implement them fully or partially, depending on their unique needs and requirements."
  },
  {
    "objectID": "faq/what-is-algorithm-stewardship.html",
    "href": "faq/what-is-algorithm-stewardship.html",
    "title": "What is algorithm stewardship?",
    "section": "",
    "text": "Model monitoring in production blogpost via neptune.ai\n\n\n\n\n\nData Quality Testing blogpost via lakeFS"
  },
  {
    "objectID": "faq/what-is-algorithm-stewardship.html#further-reading",
    "href": "faq/what-is-algorithm-stewardship.html#further-reading",
    "title": "What is algorithm stewardship?",
    "section": "",
    "text": "Model monitoring in production blogpost via neptune.ai\n\n\n\n\n\nData Quality Testing blogpost via lakeFS"
  },
  {
    "objectID": "faq/cosmosdb.html",
    "href": "faq/cosmosdb.html",
    "title": "What is CosmosDB?",
    "section": "",
    "text": "Azure Cosmos DB is a NoSQL database service offered by Microsoft as part of the Azure cloud platform. It enables globally distributed, multi-model data storage and provides low-latency, high-throughput, and consistent performance. Key features include support for various data models and APIs, elastic scalability, high availability, and comprehensive SLAs."
  },
  {
    "objectID": "quickstart/10_prerequisites.html",
    "href": "quickstart/10_prerequisites.html",
    "title": "Prerequisites for FlowEHR",
    "section": "",
    "text": "Familiarity with Python programming for data science tasks. Much of the design of the platform is orientated to support those with strong SQL skills, or those with strong R skills, but for now you will find that the examples and the work are biased toward Python."
  },
  {
    "objectID": "quickstart/10_prerequisites.html#the-skills-you-need",
    "href": "quickstart/10_prerequisites.html#the-skills-you-need",
    "title": "Prerequisites for FlowEHR",
    "section": "",
    "text": "Familiarity with Python programming for data science tasks. Much of the design of the platform is orientated to support those with strong SQL skills, or those with strong R skills, but for now you will find that the examples and the work are biased toward Python."
  },
  {
    "objectID": "quickstart/10_prerequisites.html#the-permissions-you-need",
    "href": "quickstart/10_prerequisites.html#the-permissions-you-need",
    "title": "Prerequisites for FlowEHR",
    "section": "2 The permissions you need",
    "text": "2 The permissions you need\n\nWe assume that you are working in a direct care role and therefore have rights to access identifiable data."
  },
  {
    "objectID": "quickstart/10_prerequisites.html#the-resources-you-need",
    "href": "quickstart/10_prerequisites.html#the-resources-you-need",
    "title": "Prerequisites for FlowEHR",
    "section": "3 The resources you need",
    "text": "3 The resources you need\n\nSome time and some patience!"
  },
  {
    "objectID": "quickstart/10_prerequisites.html#recommendations",
    "href": "quickstart/10_prerequisites.html#recommendations",
    "title": "Prerequisites for FlowEHR",
    "section": "4 Recommendations",
    "text": "4 Recommendations\nWe’re not telling you to do the following, but these are the sort of things you might want to consider if you hit problems.\n\nTry using Microsoft Edge as your browser for working with the TRE. It just seems to work better with this tooling.\nPay attention to the two-factor authentication. You’re likely to end up with more than one. At the minumum, you’ll have one for NHS.net and you’ll need one for the TRE (e.g. https://uclhtredev.uksouth.cloudapp.azure.com/)\n\nIt’s going to get confusing at times. There’s a lot of new terminology. Please check-out the FAQ."
  },
  {
    "objectID": "quickstart/20_setup_vscode.html",
    "href": "quickstart/20_setup_vscode.html",
    "title": "Setting up VS Code",
    "section": "",
    "text": "Download VSCode and follow the instructions at https://code.visualstudio.com/\nOpen VS Code\nOpen a terminal window by pressing Ctrl+Shift+ or using the menus Terminal -&gt; New terminal ```sh\n```"
  },
  {
    "objectID": "quickstart/20_setup_flowehr_locally.html",
    "href": "quickstart/20_setup_flowehr_locally.html",
    "title": "Set-up & deployment",
    "section": "",
    "text": "This guide will take you through the basic set up and deployment for FlowEHR in your environment."
  },
  {
    "objectID": "quickstart/20_setup_flowehr_locally.html#get-the-repositories",
    "href": "quickstart/20_setup_flowehr_locally.html#get-the-repositories",
    "title": "Set-up & deployment",
    "section": "1 Get the repositories",
    "text": "1 Get the repositories\nRepositories: 1. FlowEHR-Data-Sources - fork or clone this repo. 1. FlowEHR - this a template repo so you create your own repo from the template and customise it to your institution."
  },
  {
    "objectID": "quickstart/20_setup_flowehr_locally.html#core-configuration",
    "href": "quickstart/20_setup_flowehr_locally.html#core-configuration",
    "title": "Set-up & deployment",
    "section": "2 Core configuration",
    "text": "2 Core configuration\n\nLocallyCI/CD\n\n\nLocal instructions\n\n\nCI/CD instructions"
  },
  {
    "objectID": "quickstart/20_setup_flowehr_locally.html#infrastructure-deployment",
    "href": "quickstart/20_setup_flowehr_locally.html#infrastructure-deployment",
    "title": "Set-up & deployment",
    "section": "3 Infrastructure deployment",
    "text": "3 Infrastructure deployment\n\nLocallyCI/CD\n\n\nLocal instructions\n\n\nCI/CD instructions"
  },
  {
    "objectID": "quickstart/20_setup_flowehr_locally.html#setting-up-app-model-serving",
    "href": "quickstart/20_setup_flowehr_locally.html#setting-up-app-model-serving",
    "title": "Set-up & deployment",
    "section": "4 Setting up App & Model Serving",
    "text": "4 Setting up App & Model Serving\nNow we have our core infrastructure deployed, we can proceed to set up the app serving layer so it’s ready to deploy and host our FlowEHR Apps.\n\n4.1 Configure a GitHub App\nFor FlowEHR to create and manage repositories in GitHub for hosting FlowEHR App code, it requires a GitHub App to authenticate.\nWe recommend creating a new GitHub Organization for containing all of the FlowEHR App repositories that will be created and managed by your FlowEHR instance - just so you’re not providing unnecessary management access to any other repositories you might have in your main org.\n\n4.1.1 Create GH app\nOnce you have the organisation you wish to use at the ready, follow these instructions to create a new GitHub App within the organisation you wish to host your new FlowEHR apps, with the following properties (leave everything else blank/default):\nName: {YOUR_ORG}-FlowEHR # or something similar (needs to be globally unique)\nHomepage URL: https://flowehr.io\nWebhook: uncheck\nPermissions:\n    Repository Permissions:\n        - Actions: Read-only\n        - Administration: Read and write\n        - Contents: Read and write\n        - Environments: Read and write\n        - Metadata: Read-only\n        - Secrets: Read and write\n        - Variables: Read and write\n        - Workflows: Read and write\n    Organization Permissions:\n        - Administration: Read and write\n        - Members: Read and write\nWhere can this GitHub App be installed?: Only on this account\n\nWhen happy, click Create GitHub App. After creation, in your app’s settings page, note down the App Id.\n\n\n4.1.2 Generate Private Key\nIn the app settings page, scroll down to near the bottom and find the Private Keys section. Click Generate a private key. This will download a PEM cert file to your machine. We’ll need this later.\n\n\n\n4.1.3 Install GH app\nOnce created, you need to the install the app to the organisation. Follow these instructions, selecting your organization and choosing All repositories.\nAfter installation, stay on the same page and check the URL. It should look like this:\n\nhttps://github.com/organizations/UCLH-FlowEHR-TestBed/settings/installations/35581991\n\nAt the end of the URL after installations/, you’ll see a number (35581991 in the example above). Record this down - it is your GitHub App Installation Id. (Believe it or not this is the easiest way to find it!)\n\n\n\n4.1.4 Update config\nDepending on whether you’re configuring this for a local dev deployment, or for CI (or both if you’d like local developers and your testing environments to share a single Organization for test apps) - update the relevant config.yaml or config.{ENVIRONMENT}.yaml with the GitHub app details in the serve block:\nserve:\n    github_owner: name of the GitHub Organisation you created/wish to use for deploying apps into\n    github_app_id: your GitHub App's \"App Id\" from earlier\n    github_app_installation_id: your GitHub App's \"Installation Id\" from earlier\n\n\n4.1.5 Store the cert\n\nLocallyCI/CD\n\n\nFor developing locally, simply find the PEM file you downloaded earlier, rename it to github.pem and drag it into this repo under the /apps directory. It will be picked up by Terraform during deployment, and is gitignored so won’t be checked in accidentally.\n\n\n\n\n\n\nNote\n\n\n\nFor other developers who want to use this same app instead of setting up their own, direct them to create and download their own private key from the GitHub App’s settings page as you did in a previous step.\n\n\n\n\nFor use in CI, copy the contents of the PEM file, and paste it into a new GitHub secret called GH_APP_CERT. The CI/CD pipeline will read this into a file to use during deployments.\n\n\n\nThat’s it for now! We’ll need all this when we come to deploying apps (which we’ll cover in the deployment section)."
  },
  {
    "objectID": "quickstart/40_features.html",
    "href": "quickstart/40_features.html",
    "title": "Feature building",
    "section": "",
    "text": "We will cover\n\nfeature building\nfeature browsing\nfeature exploration\ninput data testing\nfeature testing\n\nSee"
  },
  {
    "objectID": "quickstart/20_setup_ads.html",
    "href": "quickstart/20_setup_ads.html",
    "title": "Azure Data Studio",
    "section": "",
    "text": "Open Azure Data Studio\nClick “Create a connection”\nConnection type = Microsoft SQL Server\nServer link = sql-server-features-uclh-flowehr-ml-dev.database.windows.net\nAuthentication type = Azure Active Directory\nYou’ll be taken to a browser page - authenticate with MFA\nAzure AD tenant is UCLH Secure Data Environment\nDatabase = sql-db-features\nGive it a name e.g., “FeatureStore”\n\n \nDocs"
  },
  {
    "objectID": "quickstart/30_connecting.html",
    "href": "quickstart/30_connecting.html",
    "title": "Connections and Authentication",
    "section": "",
    "text": "You will need to make the following connections."
  },
  {
    "objectID": "quickstart/30_connecting.html#connecting-to-the-source-data",
    "href": "quickstart/30_connecting.html#connecting-to-the-source-data",
    "title": "Connections and Authentication",
    "section": "1 Connecting to the source data",
    "text": "1 Connecting to the source data"
  },
  {
    "objectID": "quickstart/30_connecting.html#connecting-to-the-feature-store",
    "href": "quickstart/30_connecting.html#connecting-to-the-feature-store",
    "title": "Connections and Authentication",
    "section": "2 Connecting to the Feature Store",
    "text": "2 Connecting to the Feature Store"
  },
  {
    "objectID": "quickstart/30_connecting.html#connecting-azure-machine-learning",
    "href": "quickstart/30_connecting.html#connecting-azure-machine-learning",
    "title": "Connections and Authentication",
    "section": "3 Connecting Azure Machine Learning",
    "text": "3 Connecting Azure Machine Learning"
  },
  {
    "objectID": "quickstart/30_connecting.html#accessing-synthetic-data",
    "href": "quickstart/30_connecting.html#accessing-synthetic-data",
    "title": "Connections and Authentication",
    "section": "4 Accessing Synthetic Data",
    "text": "4 Accessing Synthetic Data\n\nWe have built a tool (Satellite) that streams structually correct but semantically meaningless data for the purposes of development.\nWe have collaborated with colleagues at the Alan Turing Institute, and have used their SQLSynthGen tool to create a replica of data."
  },
  {
    "objectID": "quickstart/30_connecting.html#connecting-to-cosmosdb",
    "href": "quickstart/30_connecting.html#connecting-to-cosmosdb",
    "title": "Connections and Authentication",
    "section": "5 Connecting to CosmosDB",
    "text": "5 Connecting to CosmosDB"
  },
  {
    "objectID": "vignettes/vignettes.html",
    "href": "vignettes/vignettes.html",
    "title": "Notebooks",
    "section": "",
    "text": "Vignettes, and examples presented as RMarkdown or Jupyter Notebooks. See the notes on contributing if you wish to share your own work.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 4, 2023\n\n\nAccessing the EHR data from the TRE\n\n\nTom Young\n\n\n\n\nMar 4, 2023\n\n\nAccessing the DICOM service from the TRE\n\n\nTom Young\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "anatomy/monitoring.html",
    "href": "anatomy/monitoring.html",
    "title": "1 Model Performance and Data Drift Monitoring",
    "section": "",
    "text": "Monitoring models in production is crucial to detecting model performance degradation early on and ensuring that the model’s predictions remain reliable over time. Two ways to achieve this are data drift detection and model performance monitoring.\nData drift detection is an unsupervised task that involves comparing a sample of the data the model was trained on to new data batches that are sent to the model for inferencing. This comparison helps to identify changes in the distribution of inputs, which may lead to a decline in the model’s performance.\nOn the other hand, model performance monitoring is a supervised task that involves comparing the model’s output to the ground truth. This task is useful in identifying changes in the model’s behavior when the ground truth is available.\nIt is important to note that maintaining a data drift detector even when the ground truth is available has two benefits. Firstly, it can detect changes earlier than relying solely on the ground truth. Secondly, it helps to understand the cause of model degradation even when the ground truth is available.\n\n\n\nIn the provided template, we assume the model will be using tabular data with ontinuous and categorical variables.\n\n\n\nThe solution requires that Azure ML and Azure Devops instances will be available in the production environment.\n\n\n\nWhen a model is ready to be pushed to production, it will be the data scientists’ responsibility to provide the code required for monitoring, by altering the provided monitoring template (see the Monitoring Template section of this document for more details).\n\n\n\nThe monitoring workflow is depicted in the image below. The document assumes that all the required data is stored in Cosmos DB and readily available for use.\n\n\n\n‘flowchart’\n\n\n\n\nThe monitoring tool uses four data sources in order to run the pipelines: * GroundTruth: contains the actual labels of the inferred data. This data will only be available after providing the model’s predictions, and will be used to evaluate the performance of teh model. * InferredData: contains the predictions made by the ML model * UnlabledData: contains the new data that needs to be inferred by the model * TrainingSample: a sample of the training set (copied from the TRE)\n\n\n\nThe Azure Pipelines will trigger the AML pipelines when new data becomes availabe, or on a predefined schedule. The Azure pipeline receives all the environment variables required to run Azure ML (such as subscription ID, client ID etc.)\n\n\n\nIn Azure ML, we will have two pipelines - one for model performance monitoing and another one for drift detection.\n\n\nInputs: model predictions and their corresponding ground truths (when they become available) The script will compute the model’s performance by comparing the predictions that were made by the model to the ground truth.\n\n\n\nInputs: A sample of the training data, a set of new data Output: For each feature, the script will determine whether a data drift occurred or not.\nIn this example, we used the alibi-detect open source library to compute the data drift, but the data scientist can decide on the best monitoring tools based on the use case [^1] [^1]: For images, we could also use Torchdrift or theDeepchecks open source library\nIf we would also like to support label drift, in addition to the training set, we will also need to save the predictions the model made on the training data so that we can compare the distribution of the predictions.\nThe pipelines record the selected metrics and graphs using MLFlow and also send metrics to Azure Monitor\n\n\n\n\nIn Azure Monitor, we will define queries and use them to set alerts and create dashboards. The alerts can be linked to email accounts (or to a Teams channel) of the data scientist responsible for the model. When an alert is triggered, the data scientist can view the monitoring dashboard to get some more information about what has trigerred it.\n\n\n\n\nThe monitoring template is meant to help the data scientist create the artifacts required to run the monitoring workflow. To setup the workflow, the data scientist will need to follow the following steps:\n\n\nIn the data_drift and model_performance folders, edit the source code (e.g data_drift/data_drift_src/data_drift.py) to define the monitoring functions. * For data drift, the template supports running Kolmogorov-Smirnov algorithm for all continuous variables and Chi-squared tests for all categorical variables. ### Step 2: Configure the data paths and compute Change the config.json file to point to the location of the data sources and to select the compute name (the assumption is that a compute has already been created in AML) ### Step 3: Create a client application In order for Azure Pipelines to trigger AML, you will need to create a client application in Azure and add a client secret. ### Step 4: Configure Azure DevOps * Start a new Azure DevOps project, connect it to the template repo , and create a new piplie using the azure-pipelines.yml yaml file (see this link for more details). * Link Azure DevOps to Azure KeyVault and define the following environment variables: * Azure_Client_ID: the ID of the client appliaction * AZURE_CLIENT_SECRET: the secret of the client application * AZURE_LOG_HANDLER_CONNECTION_STRING: connection string used to log metrics to Azure Moniring (see here for more details) * AzureML_Workspace_name: the name of the AML workspace where the experiments will be ran * Resource_Group: the name of teh resource group * Subscription_ID: the user’s subscription ID * Tenant_ID: the tenant ID ### Step 5: Run the pipeline When the run completes, the artifacts will be stored in the AML run.\nThe logs that were saved to Azure Monitor are stored in the traces table. They can be queried using Kusto Query Language (KQL)\nFor example, run the following query to detect if a drift has occurred:\ntraces\n| where message == 'data_drift_total'\n| project toreal(customDimensions.is_drift)\nThese queries can be used to create alerts and add tiles to Azure dashboards\n\n\n\n\n\nHow to read data from Cosmos DB to Azure blob storage? what should the query look like?\nWhen to trigger the pipelined -\n\nwhenever there is a change in Cosmos DB datasets\nat pre-defined intervals"
  },
  {
    "objectID": "anatomy/monitoring.html#introduction",
    "href": "anatomy/monitoring.html#introduction",
    "title": "1 Model Performance and Data Drift Monitoring",
    "section": "",
    "text": "Monitoring models in production is crucial to detecting model performance degradation early on and ensuring that the model’s predictions remain reliable over time. Two ways to achieve this are data drift detection and model performance monitoring.\nData drift detection is an unsupervised task that involves comparing a sample of the data the model was trained on to new data batches that are sent to the model for inferencing. This comparison helps to identify changes in the distribution of inputs, which may lead to a decline in the model’s performance.\nOn the other hand, model performance monitoring is a supervised task that involves comparing the model’s output to the ground truth. This task is useful in identifying changes in the model’s behavior when the ground truth is available.\nIt is important to note that maintaining a data drift detector even when the ground truth is available has two benefits. Firstly, it can detect changes earlier than relying solely on the ground truth. Secondly, it helps to understand the cause of model degradation even when the ground truth is available."
  },
  {
    "objectID": "anatomy/monitoring.html#the-use-case",
    "href": "anatomy/monitoring.html#the-use-case",
    "title": "1 Model Performance and Data Drift Monitoring",
    "section": "",
    "text": "In the provided template, we assume the model will be using tabular data with ontinuous and categorical variables."
  },
  {
    "objectID": "anatomy/monitoring.html#requirements",
    "href": "anatomy/monitoring.html#requirements",
    "title": "1 Model Performance and Data Drift Monitoring",
    "section": "",
    "text": "The solution requires that Azure ML and Azure Devops instances will be available in the production environment."
  },
  {
    "objectID": "anatomy/monitoring.html#before-deploying-a-model",
    "href": "anatomy/monitoring.html#before-deploying-a-model",
    "title": "1 Model Performance and Data Drift Monitoring",
    "section": "",
    "text": "When a model is ready to be pushed to production, it will be the data scientists’ responsibility to provide the code required for monitoring, by altering the provided monitoring template (see the Monitoring Template section of this document for more details)."
  },
  {
    "objectID": "anatomy/monitoring.html#monitoring-design",
    "href": "anatomy/monitoring.html#monitoring-design",
    "title": "1 Model Performance and Data Drift Monitoring",
    "section": "",
    "text": "The monitoring workflow is depicted in the image below. The document assumes that all the required data is stored in Cosmos DB and readily available for use.\n\n\n\n‘flowchart’\n\n\n\n\nThe monitoring tool uses four data sources in order to run the pipelines: * GroundTruth: contains the actual labels of the inferred data. This data will only be available after providing the model’s predictions, and will be used to evaluate the performance of teh model. * InferredData: contains the predictions made by the ML model * UnlabledData: contains the new data that needs to be inferred by the model * TrainingSample: a sample of the training set (copied from the TRE)\n\n\n\nThe Azure Pipelines will trigger the AML pipelines when new data becomes availabe, or on a predefined schedule. The Azure pipeline receives all the environment variables required to run Azure ML (such as subscription ID, client ID etc.)\n\n\n\nIn Azure ML, we will have two pipelines - one for model performance monitoing and another one for drift detection.\n\n\nInputs: model predictions and their corresponding ground truths (when they become available) The script will compute the model’s performance by comparing the predictions that were made by the model to the ground truth.\n\n\n\nInputs: A sample of the training data, a set of new data Output: For each feature, the script will determine whether a data drift occurred or not.\nIn this example, we used the alibi-detect open source library to compute the data drift, but the data scientist can decide on the best monitoring tools based on the use case [^1] [^1]: For images, we could also use Torchdrift or theDeepchecks open source library\nIf we would also like to support label drift, in addition to the training set, we will also need to save the predictions the model made on the training data so that we can compare the distribution of the predictions.\nThe pipelines record the selected metrics and graphs using MLFlow and also send metrics to Azure Monitor\n\n\n\n\nIn Azure Monitor, we will define queries and use them to set alerts and create dashboards. The alerts can be linked to email accounts (or to a Teams channel) of the data scientist responsible for the model. When an alert is triggered, the data scientist can view the monitoring dashboard to get some more information about what has trigerred it."
  },
  {
    "objectID": "anatomy/monitoring.html#monitoring-template",
    "href": "anatomy/monitoring.html#monitoring-template",
    "title": "1 Model Performance and Data Drift Monitoring",
    "section": "",
    "text": "The monitoring template is meant to help the data scientist create the artifacts required to run the monitoring workflow. To setup the workflow, the data scientist will need to follow the following steps:\n\n\nIn the data_drift and model_performance folders, edit the source code (e.g data_drift/data_drift_src/data_drift.py) to define the monitoring functions. * For data drift, the template supports running Kolmogorov-Smirnov algorithm for all continuous variables and Chi-squared tests for all categorical variables. ### Step 2: Configure the data paths and compute Change the config.json file to point to the location of the data sources and to select the compute name (the assumption is that a compute has already been created in AML) ### Step 3: Create a client application In order for Azure Pipelines to trigger AML, you will need to create a client application in Azure and add a client secret. ### Step 4: Configure Azure DevOps * Start a new Azure DevOps project, connect it to the template repo , and create a new piplie using the azure-pipelines.yml yaml file (see this link for more details). * Link Azure DevOps to Azure KeyVault and define the following environment variables: * Azure_Client_ID: the ID of the client appliaction * AZURE_CLIENT_SECRET: the secret of the client application * AZURE_LOG_HANDLER_CONNECTION_STRING: connection string used to log metrics to Azure Moniring (see here for more details) * AzureML_Workspace_name: the name of the AML workspace where the experiments will be ran * Resource_Group: the name of teh resource group * Subscription_ID: the user’s subscription ID * Tenant_ID: the tenant ID ### Step 5: Run the pipeline When the run completes, the artifacts will be stored in the AML run.\nThe logs that were saved to Azure Monitor are stored in the traces table. They can be queried using Kusto Query Language (KQL)\nFor example, run the following query to detect if a drift has occurred:\ntraces\n| where message == 'data_drift_total'\n| project toreal(customDimensions.is_drift)\nThese queries can be used to create alerts and add tiles to Azure dashboards"
  },
  {
    "objectID": "anatomy/monitoring.html#tbd",
    "href": "anatomy/monitoring.html#tbd",
    "title": "1 Model Performance and Data Drift Monitoring",
    "section": "",
    "text": "How to read data from Cosmos DB to Azure blob storage? what should the query look like?\nWhen to trigger the pipelined -\n\nwhenever there is a change in Cosmos DB datasets\nat pre-defined intervals"
  },
  {
    "objectID": "anatomy/feature_store.html",
    "href": "anatomy/feature_store.html",
    "title": "The FlowEHR feature store",
    "section": "",
    "text": "A feature store is a centralized repository designed to manage, store, and serve features for machine learning (ML) models. Features are the transformed and engineered attributes derived from raw data that are used as inputs for ML models. Feature stores play a critical role in the machine learning workflow by providing a consistent and reusable way to store and share these features across multiple models and teams.\nThe main components of a feature store typically include:\n\nFeature Engineering: This involves transforming raw data into useful features that can be used as input for ML models. Feature engineering may include operations like aggregation, normalization, and encoding.\nFeature Storage: A feature store provides a scalable and efficient storage system to store both historical and real-time features, allowing for the retrieval of feature data in a consistent manner for training and serving purposes.\nFeature Serving: Feature stores enable serving features for both model training and inference. They provide low-latency access to feature data for real-time model predictions and batch access for training purposes.\nFeature Sharing and Discovery: A feature store allows data scientists and ML engineers to share and reuse features across multiple models and teams. This promotes collaboration and reduces redundant work in feature engineering.\nFeature Monitoring and Management: Feature stores help monitor and manage feature quality, such as tracking feature statistics, detecting data drift, and ensuring data consistency.\n\nBy centralizing the management of features, a feature store simplifies the ML workflow, promotes collaboration, reduces redundancy, and ensures consistency across models. Additionally, it helps maintain the quality and reliability of the features used in ML models, leading to more accurate and reliable predictions."
  },
  {
    "objectID": "anatomy/feature_store.html#overview",
    "href": "anatomy/feature_store.html#overview",
    "title": "The FlowEHR feature store",
    "section": "",
    "text": "A feature store is a centralized repository designed to manage, store, and serve features for machine learning (ML) models. Features are the transformed and engineered attributes derived from raw data that are used as inputs for ML models. Feature stores play a critical role in the machine learning workflow by providing a consistent and reusable way to store and share these features across multiple models and teams.\nThe main components of a feature store typically include:\n\nFeature Engineering: This involves transforming raw data into useful features that can be used as input for ML models. Feature engineering may include operations like aggregation, normalization, and encoding.\nFeature Storage: A feature store provides a scalable and efficient storage system to store both historical and real-time features, allowing for the retrieval of feature data in a consistent manner for training and serving purposes.\nFeature Serving: Feature stores enable serving features for both model training and inference. They provide low-latency access to feature data for real-time model predictions and batch access for training purposes.\nFeature Sharing and Discovery: A feature store allows data scientists and ML engineers to share and reuse features across multiple models and teams. This promotes collaboration and reduces redundant work in feature engineering.\nFeature Monitoring and Management: Feature stores help monitor and manage feature quality, such as tracking feature statistics, detecting data drift, and ensuring data consistency.\n\nBy centralizing the management of features, a feature store simplifies the ML workflow, promotes collaboration, reduces redundancy, and ensures consistency across models. Additionally, it helps maintain the quality and reliability of the features used in ML models, leading to more accurate and reliable predictions."
  },
  {
    "objectID": "anatomy/feature_store.html#our-implementation",
    "href": "anatomy/feature_store.html#our-implementation",
    "title": "The FlowEHR feature store",
    "section": "2 Our implementation",
    "text": "2 Our implementation\nWe store all features in a Microsoft SQL database, and for now have focused on the design of the feature generation pipeline. We have not (yet) implemented separate approaches for realtime time serving (for predictions) and batch serving (for training). As our data volumes and user base grow, we will review this decision.\nWe chose MSSQL to ensure access to the database for the widest possible community within the NHS.\nThe feature pipeline is managed by the Data Forge repository.\nThe key components of this include:\n\nA pipeline configuration\n\nThis defines which features are built, and specifies specific configuration. See pipeline.json\n\nA pipeline trigger\n\nThis defines the schedule at which features are updated. See trigger.json.\n\nA per feature entry point\n\nSee bloodpressure.py that generates a feature for both the latest, and the average blood pressure (where the 24 hour aggregation window is defined in pipeline.json). The are regenerated hourly as per the trigger.json configuration.\n\nA per feature ‘job’\n\nSee bloodpressure/job.py that fetches the source data, splits the text representation 120/70 into integers for systolic and diastolic pressures,\n\n\nEach feature is keyed on an identifier for the hospital visit, and the horizon_datetime is the timestamp derived from the trigger against which all these features are anchored."
  },
  {
    "objectID": "anatomy/gitea.html",
    "href": "anatomy/gitea.html",
    "title": "Version Control (Gitea)",
    "section": "",
    "text": "We use Gitea for version control, and collaboration within the TRE. Think of this as a secure and private GitHub accessible to all users within the TRE, but unable to reach the outside world without special permissions.\nThe Gitea docs explain"
  },
  {
    "objectID": "anatomy/gitea.html#gitea-deployment-models-and-features",
    "href": "anatomy/gitea.html#gitea-deployment-models-and-features",
    "title": "Version Control (Gitea)",
    "section": "1 Gitea Deployment Models and Features",
    "text": "1 Gitea Deployment Models and Features\nThe Gitea service in the TRE allows deployment at two levels:\n\nGitea as a TRE Shared Service\n\nprovides a Gitea instance that is accessible from any TRE Workspace\nsupports pull-mirrors: code held on Git Repositories external to the TRE can be mirrored in the Gitea TRE Shared Service\nsupports push-mirrors: code held on the Gitea TRE Shared Service can be mirrored to Git Repositories external to the TRE\npush or pull mirrors can be configured by any user with an account on the Gitea TRE Shared Service\nby default, only one account (giteaadmin) is configured with access to the Gitea TRE Shared Service\nWorkspace users are unable to create accounts on the server\n\nGitea as an individual Workspace Service\n\nprovides a Gitea instance only accessible from the individual TRE Workspace\nno support for creation of pull-mirrors\nno support for creation of push-mirrors\nallows Workspace users to create accounts on the server using OpenID\nusers with an account can create repositories that are accessible to other users in the same Workspace\n\n\nPush-mirrors expose the TRE Workspaces to accidental exfiltration of data from a TRE Workspace to the Internet. Since push-mirrors can only be created on the Gitea TRE Shared Service, Workspace users will not be provided with accounts on this service.\nPull-mirrors can be created for Public or Private repositories on the Gitea TRE Shared Service. Mirrors for Private repositories need an Access Token to retrieve the Private repository from the internet Git server. Gitea mirrors of private, external repositories can be either Public or Private. If public, then all users from all Workspaces will be able to read the repository. If the repository needs to be kept private to a specific Workspace or Workspace user, then a Gitea resticted account can be configured on the Gitea TRE Shared Service. This restricted account can be shared with the intended audience, granting them read-access to the Private repository only. Restricted accounts are unable to see public respositories on the Gitea server, unless they have been explicitly granted permissions to do so.\nPull-mirrors can be created as a snapshot of the current state of an external repository or can be configured to synchronise with the external repository on a schedule."
  },
  {
    "objectID": "anatomy/gitea.html#gitea-service-requirements",
    "href": "anatomy/gitea.html#gitea-service-requirements",
    "title": "Version Control (Gitea)",
    "section": "2 Gitea Service Requirements",
    "text": "2 Gitea Service Requirements\nThe Gitea services in the TRE should support the following requirements:\n\nAccess to read and write code and share this with colleagues using the same TRE Workspace\nAccess to publish code, subject to approval, outside of the TRE Workspaces\nAccess to read code that is currently held outside of the TRE\nAbility to read code held in a private, external repository and keep the code visibility private\nWhen pushing code to a repository, appropriate controls are applied to prevent inadvertently publishing sensitive data beyond the confines of the Workspace private network\nPrevent users from adding push mirrors to their repositories"
  },
  {
    "objectID": "anatomy/gitea.html#recommendations",
    "href": "anatomy/gitea.html#recommendations",
    "title": "Version Control (Gitea)",
    "section": "3 Recommendations",
    "text": "3 Recommendations\n\nA Gitea TRE Shared Service should be deployed to provide read-only access across Workspaces to approved Git Repositories held outside of the TRE\nPull-mirrors on the Gitea TRE Shared Service should only be configurable by the TRE administrators (via the giteaadmin account)\nPull-mirrors should be organised into Organisations, to indicate the Workspace that originally requested the mirror to be created\nWhere a private pull-mirror is required, a restricted user can be provisioned to allow the intended audience to read from the private mirror only. The restricted user account should also have the following settings:\n\nvisibility: private\nmax number of repositories: 0\ndisable sign-in: No\nis administrator: No\nmay create organisations: No\nfull name: name of Workspace owner\nemail address: email address of Workspace owner\n\nNo additional logon accounts will be configured for the Gitea TRE Shared Service, except where Push Mirrors have been approved\nGitea Workspace Services should be deployed to each Workspace to enable Workspace users to collaborate within the Workspace using read-write repositories\nWhen Workspace users wish to move code from a Workspace repository, they will need to complete an approvals process. At the moment this process would involve:\n\ndownloading the repository code to a Workspace VM\ncompleting an ‘airlock’ review process to move the downloaded code to a device outside the TRE"
  },
  {
    "objectID": "anatomy/gitea.html#gitea-push-mirrors",
    "href": "anatomy/gitea.html#gitea-push-mirrors",
    "title": "Version Control (Gitea)",
    "section": "4 Gitea Push Mirrors",
    "text": "4 Gitea Push Mirrors\nIn exceptional circumstances, Push-Mirrors can be configured on the Gitea TRE Shared Service. Push-Mirrors add the risk of sensitive information leaving the TRE Workspace. For this reason, access to Push-Mirrors will be closely controlled. The following controls will be applied to mitigate the risk of data exfiltration:\n\nPush-Mirrors will be created on the Gitea TRE Shared Service\nOnly the Gitea admin account will have permission to create a Push-Mirror\nPush-Mirrors will be created within an Organisation\nThe Organisation name will reference the TRE Workspace name\nUsers from the Workspace will be provided with restricted logons to the Gitea TRE Shared Service. The restricted logons will have the following settings:\n\nmax number of repositories: 0\ndisable sign-in: No\nis administrator: No\nmay create organisations: No\nfull name: name of Workspace owner\nemail address: email address of Workspace owner\n\nRestricted accounts will be granted write access to their Organisation repositories by membership of the Organisation’s writer group\nPush settings will be set as:\n\nscheduled for Private repositories held in approved Github Organisations\non-demand for all other repositories\n\nOn-demand push will only be executable by the giteaadmin and will require authorisation from Organisation owners"
  },
  {
    "objectID": "anatomy/apps.html",
    "href": "anatomy/apps.html",
    "title": "FlowEHR Apps",
    "section": "",
    "text": "What are FlowEHR Apps?"
  }
]