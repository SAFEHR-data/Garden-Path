[
  {
    "objectID": "whoami/personas.html",
    "href": "whoami/personas.html",
    "title": "Personas",
    "section": "",
    "text": "Infrastructure Engineer: Creates Azure infrastructure via Terraform code\nData Engineer: Writes data transformation code\nInternal Innovators:\n\nApp Developer: Uses a synthetic data store to build apps + dashboards\nData Scientist: Uses a TRE to build ML models against patient data\n\nSteward: Monitors and manages ML models and apps"
  },
  {
    "objectID": "data-scientist/onboarding.html",
    "href": "data-scientist/onboarding.html",
    "title": "Data science onboarding",
    "section": "",
    "text": "Use authenticator app - can be any authenticator app, google or microsoft\nRequires to have an NHS.net email. If you do not have access this needs to be arranged with an UCLH/Azure TRE Administrator\nView the available workspaces\n\nThis is a shared area commonly for people to collaborate on a project together.\nThis has shared storage"
  },
  {
    "objectID": "data-scientist/onboarding.html#create-machine",
    "href": "data-scientist/onboarding.html#create-machine",
    "title": "Data science onboarding",
    "section": "Create machine",
    "text": "Create machine\n\nClick virtual machines (VM) to go into the personal list of available machines. Do not click on Connect as this will take you to a different place.\nCreate a VM with the specified ID - this can be anything of your choosing as this is only visible to you.\nUse the machine with the lowest resources (CPU and RAM) that facilitates your work. This can always be upgraded later if you need. We recommend starting at CPU=2, RAM=8GB\nWait for your VM to load, this might take a couple of minutes. You might need to refresh your screen to ensure that the VM has been deployed/provisioned/available."
  },
  {
    "objectID": "data-scientist/onboarding.html#python",
    "href": "data-scientist/onboarding.html#python",
    "title": "Data science onboarding",
    "section": "Python",
    "text": "Python\n\nConda Environments\n\nConda has been configured to use a local mirror of conda and conda-forge in a framework called Nexus. This means that you will have access to most, but not all, packages and some of them will be outdated.\nThere are some conda environments that have already been made with pytorch/tensorflow/automl. Ideally those should be used in the first instance and packages can be added to that.\nIf you wish to create your own environment, ensure that you have activated your base conda environment using conda activate prior to creating any other conda environments\nIf you do wish to install other conda-like installers (e.g. mamba), ensure that the base conda environment has been activated so that the configuration files for Nexus has migrated into the mamba configuration.\n\nYou’ll probably have more success with conda-forge as your default channel as Nexus mirrors that more successfully. To do this, run: conda config --add channels conda-forge\n\nOn first use, jump into a terminal and conda init\n\n\n\nPip Environments\n\nPip environments have also been configured to work with Nexus.\nThis means that any pip install command should work out of the box\nWe advise that you create a conda environment (as above), and install any packages that are not in conda using pip within that conda environment"
  },
  {
    "objectID": "data-scientist/onboarding.html#rrstudio",
    "href": "data-scientist/onboarding.html#rrstudio",
    "title": "Data science onboarding",
    "section": "R/RStudio",
    "text": "R/RStudio\n\nThere is an installation of R (version 4.1) and RStudio with basic packages.\nA mirror of packages available on CRAN has been developed and thus the installation of packages is possible:\n\nSpecifically, we’ve tested rstan and brms installations as they require downloading and compilation capabilities."
  },
  {
    "objectID": "data-scientist/onboarding.html#airlock",
    "href": "data-scientist/onboarding.html#airlock",
    "title": "Data science onboarding",
    "section": "Airlock",
    "text": "Airlock\nTo transfer files into the workspace/VM - there are two things you’ll need: 1. An airlock transfer request which gives you a link to a Storage Blob that is a temporary container link (SAS) to upload to 2. Microsoft Azure Storage Explorer app - this is available for Windows, MacOS, and Linux (as a snap and a tar)\n\nImport File Transfer\n\nEnsure that the file you wish to transfer is a single file - if you wish to transfer many files, zip them.\nIf you have not downloaded the Microsoft Azure Storage Explorer installed, do so now.\nClick on Airlock on the menu in the Azure TRE\nYou’ll be greeted with a list of current active requests (if any).\nTo start the process, click on New Request and then click on import - this will open a dialog on the right where you can name and describe your airlock transfer request\nOnce you’ve created the transfer request, you’ll be greeted with another dialog on the right describing the request. Copy the SAS URL - this step is important\nOpen the Microsoft Azure Storage Explorer. You’ll be greeted with a Get Started Page called Storage Explorer. If this is not available, exit and open it again or open a new window. Failing that, click on Help, then Reset\nClick on Attach to a resource - this will open a new window titled Select a Resource\nClick on Blob container\nThen select Shared access signature URL (SAS), then click Next\nThen past the SAS URL that copied from the Airlock transfer window in the Azure TRE into the blob container SAS URL. The Display name will be populated automatically.\nClick Next, and then Connect. After a moment, you will be greeted with a new Tab which has a Upload button on the top left hand side. Click that and select Upload Files...\nThis will open another dialog box titled Upload Files. Click on the ... next to the “No Files Selected”\nClick Upload. This will take a little while depending on the size of the file and the speed of your connection.\nGo back to the Azure TRE, you should still have the dialog with the Airlock open on the right hand size. If you have closed it, you can double click on the title of your upload that you created. Click on Submit.\nThis will now require a person to approve the ingress of the data into the Azure TRE. You can view the progress of this from this screen.\nOnce approved, your files will be located in the workspace"
  },
  {
    "objectID": "data-scientist/onboarding.html#integrated-development-environments-ides",
    "href": "data-scientist/onboarding.html#integrated-development-environments-ides",
    "title": "Data science onboarding",
    "section": "Integrated Development Environments (IDEs)",
    "text": "Integrated Development Environments (IDEs)\n\nVisual Studio Code\nThere is an installation of Visual Studio Code but has limited functionality for extensions due to security reasons. It also does not have access to Remote Extensions and thus docker installations within Remote Extensions is not currently possible.\n\n\nPyCharm Community Edition\nThere is an installation of PyCharm, it is the community edition and is thus limited in scope and it also does not have support for external extensions for the security reasons."
  },
  {
    "objectID": "data-scientist/onboarding.html#assorted-notes-and-tips",
    "href": "data-scientist/onboarding.html#assorted-notes-and-tips",
    "title": "Data science onboarding",
    "section": "Assorted notes and tips",
    "text": "Assorted notes and tips\n\nshared file system at /vm-shared-storage/\nwhen logging in to Azure ML within the TRE then you need to use the MFA token from https://portal.azure.com/#home not from NHS email"
  },
  {
    "objectID": "quickstart/10_prerequisites.html",
    "href": "quickstart/10_prerequisites.html",
    "title": "Prerequisites for FlowEHR",
    "section": "",
    "text": "The skills you need\nThe permissions you need\nThe resources you need"
  },
  {
    "objectID": "quickstart/20_setup.html",
    "href": "quickstart/20_setup.html",
    "title": "Set-up",
    "section": "",
    "text": "How to set-up FlowEHR on Azure"
  },
  {
    "objectID": "faq/faq.html",
    "href": "faq/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Please browse our FAQs, and short ‘today I learned’ posts.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nHow do I contribute to the documentation\n\n\nSteve Harris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "faq/contributing.html",
    "href": "faq/contributing.html",
    "title": "How do I contribute to the documentation",
    "section": "",
    "text": "We welcome new contributions, edits to existing material, and ad hoc supporting material. The latter category could include example notebooks or vignettes showing how something works, or a quick note.\n\nNotebooks and vignettes can just be saved in ./docs/notebooks/. Please ensure the first cell in the notebook contains the YAML metadata for the author, title, and categories.\nQuick notes in the style FAQ answers, or ‘today I learned’ notes can be saved into ./docs/faq/\n\nBoth will be automatically indexed when the site is deployed. The documentation is deployed using Quarto, and the documentation is built from the ./docs/ directory. Please fork or clone .\nYou will need to have\n\ninstalled basic python and jupyter extensions necessary including those for jupyter\ninstalled quarto\ninstalled the VSCode extension\n\nWorkflow\ncd docs\nquarto preview\nThen create your content and review before making a pull request. Don’t forget that changes to config (e.g. _quarto.yml may need you to rerun quarto render to ensure the whole site is correctly rebuilt) When you push to either dev or main branches, then a GitHub Action (see .github/workflows/publish.yml) will be triggered that should publish the current documentation to https://hylode.github.io/HyUi/."
  },
  {
    "objectID": "vignettes/dicom_access.html",
    "href": "vignettes/dicom_access.html",
    "title": "Accessing the DICOM service from the TRE",
    "section": "",
    "text": "Note: DICOM data accessed through this service has been anonymised to some degree and will not mirror data from the original source.\n\nPII has been removed\nDates have been moved\n\n\nimport requests\nimport pydicom\nfrom pathlib import Path\nfrom urllib3.filepost import encode_multipart_formdata, choose_boundary\nfrom azure.identity import DefaultAzureCredential\n\n\n\n\nservice_url=\"https://hdsflowehrdev-dicom-flowehr-dev.dicom.azurehealthcareapis.com\"\nversion=\"v1\"\nbase_url = f\"{service_url}/{version}\"\nprint(service_url)\n\n\n\n\nEnter the provided code in a browser outside of the TRE VM\n\n!az login --use-device-code\n\nEnsure the correct subscription is set as the ‘default’ subscription. Please select the subscription name you would like to use for futher authentication against the DICOM service from the list of subscriptions returned by the previous cell.\nReplace your-subscription-name with the actual subscription name in the below cell and run the cell.\n\n!az account set --subscription \"your-subscription-name\"\n\n\n\n\n\nfrom azure.identity import DefaultAzureCredential, AzureCliCredential\ncredential = DefaultAzureCredential()\ntoken = credential.credentials[3].get_token('https://dicom.healthcareapis.azure.com')\nbearer_token = f'Bearer {token.token}'\n\n\n\n\nGenerates an equivalent token to the above cell, may be used if problems with DefaultAzureCredential are encountered.\n\ncredential = AzureCliCredential()\nbearer_token = f\"Bearer {credential.get_token('https://dicom.healthcareapis.azure.com').token}\"\n\n\n\n\n\nclient = requests.session()\n\n\n\n\n\nheaders = {\"Authorization\":bearer_token}\nurl= f'{base_url}/changefeed'\n\nresponse = client.get(url,headers=headers)\nif (response.status_code != 200):\n    print('Error! Likely not authenticated!')\nprint(response.status_code)\n\n\n\n\n\n\n\nurl = f\"{base_url}/studies\"\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nresponse_query = client.get(url, headers=headers)\nprint(f\"{response_query.status_code=}, {response_query.content=}\")\n\nExtract study IDs from response content - returned as bytes\nStudyInstanceUID corresponds to 0020000D - see https://dicom.nema.org/medical/dicom/current/output/html/part18.html\n\nimport json\nr = json.loads(response_query.content.decode())\nstudy_uids = [study[\"0020000D\"][\"Value\"][0] for study in r]\n\n\n\n\n\nstudy_uid = study_uids[0] # as an example, use the previous query\nurl = f\"{base_url}/studies\"\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nparams = {'StudyInstanceUID':study_uid}\nresponse_query = client.get(url, headers=headers, params=params)\nprint(f\"{response_query.status_code=}, {response_query.content=}\")\n\nReturn series UIDs within a single study\n\nurl = f'{base_url}/studies/{study_uids[0]}/series'\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nresponse = client.get(url, headers=headers)\nprint(f\"{response.status_code=}, {response.content=}\")\n\nSearch within study by series UID\n\nurl = f'{base_url}/studies/{study_uid}/series'\nheaders = {'Accept':'application/dicom+json', \"Authorization\":bearer_token}\nparams = {'SeriesInstanceUID':series_uid}\n\nresponse = client.get(url, headers=headers, params=params) #, verify=False)\nprint(f\"{response.status_code=}, {response.content=}\")\n\nSearch all studies by series UID\n\nurl = f'{base_url}/series'\nheaders = {'Accept': 'application/dicom+json', \"Authorization\":bearer_token}\nparams = {'SeriesInstanceUID': series_uid}\nresponse = client.get(url, headers=headers, params=params)\nprint(f\"{response.status_code=}, {response.content=}\")\n\n\n\n\n\n\n\n\nurl = f'{base_url}/studies/{study_uid}'\nheaders = {'Accept':'multipart/related; type=\"application/dicom\"; transfer-syntax=*', \"Authorization\":bearer_token}\n\nresponse = client.get(url, headers=headers)\n\nInstances are retrieved as bytes - to return useful output, we’ll loop through returned items and convert to files that can be read by pydicom\n\nimport requests_toolbelt as tb\nfrom io import BytesIO\n\nmpd = tb.MultipartDecoder.from_response(response)\n\nretrieved_dcm_files = []\nfor part in mpd.parts:\n    # headers returned as binary\n    print(part.headers[b'content-type'])\n    \n    dcm = pydicom.dcmread(BytesIO(part.content))\n    print(dcm.PatientName)\n    print(dcm.SOPInstanceUID)\n    retrieved_dcm_files.append(dcm)\n\n\nprint(retrieved_dcm_files[0].file_meta)\n\n\n\n\n\nresponse_array = []\nfor study_uid in study_uids:\n    url = f'{base_url}/studies/{study_uid}'\n    headers = {'Accept':'multipart/related; type=\"application/dicom\"; transfer-syntax=*', \"Authorization\":bearer_token}\n    response = client.get(url, headers=headers)\n    response_array.append(response)\n\nParse returned items and output a list of lists, with a list of instances per study\n\nimport requests_toolbelt as tb\nfrom io import BytesIO\n\nretrieved_dcm_files_multistudy = []\nfor r in response_array:\n    mpd = tb.MultipartDecoder.from_response(r)\n\n    retrieved_study_dcm_files = []\n    for part in mpd.parts:\n        dcm = pydicom.dcmread(BytesIO(part.content))\n        retrieved_study_dcm_files.append(dcm)\n    retrieved_dcm_files_multistudy.append(retrieved_study_dcm_files)\n\n\nprint(retrieved_dcm_files_multistudy[0][0].file_meta)\n\n\n\n\n\nInstance images can be viewed by plotting the pixel array with matplotlib (or a similar library)\n\nimport matplotlib.pyplot as plt\nplt.imshow(retrieved_dcm_files[0].pixel_array, cmap=plt.cm.bone)\n\n\n\n\n\nurl = f'{base_url}/studies/{study_uid}/series/{series_uid}/instances/{instance_uid}'\nheaders = {'Accept':'application/dicom; transfer-syntax=*', \"Authorization\":bearer_token}\n\nresponse = client.get(url, headers=headers)\n\nAgain, the single instance is returned as bytes, which we can pass to pydicom with\n\ndicom_file = pydicom.dcmread(BytesIO(response.content))\nprint(dicom_file.PatientName)\nprint(dicom_file.SOPInstanceUID)\nprint(dicom_file.file_meta)\n\n\nplt.imshow(dicom_file.pixel_array, cmap=plt.cm.bone)"
  },
  {
    "objectID": "vignettes/vignettes.html",
    "href": "vignettes/vignettes.html",
    "title": "Notebooks",
    "section": "",
    "text": "Vignettes, and examples presented as RMarkdown or Jupyter Notebooks. See the notes on contributing if you wish to share your own work.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 4, 2023\n\n\nAccessing the EHR data from the TRE\n\n\nTom Young\n\n\n\n\nMar 4, 2023\n\n\nAccessing the DICOM service from the TRE\n\n\nTom Young\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "vignettes/data_lake_access.html",
    "href": "vignettes/data_lake_access.html",
    "title": "Accessing the EHR data from the TRE",
    "section": "",
    "text": "This notebook describes the process of accessing EHR data stored within a TRE-accessible Azure Data Lake Storage Gen2 (ADLS) instance.\nNote that PII has been masked from reports stored in the TRE and as a consequence structural changes may have appeared with respect to the original data.\nFeatures intended for use as inputs to machine learning models should not include or be derived from structural information from reports (e.g. line breaks, sentence length) stored in the storage instance detailed below.\n\n\nOpen the link shown in a browser outside of the TRE, enter the code and log in with your user account\n\n!az login --use-device-code\n\n\n\n\n\nstorage_account_name = \"stflowehrdev\"\ninput_data_fs_name=\"data-lake-storage-flowehr-dev\"\ndata_directory_path=\"GoldZone/patients/\"\n\nImport dependencies and define functions to query data\n\n#Function definitions inspired by MS docs\n#at https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-directory-file-acl-python\nimport os, uuid, sys\nfrom azure.storage.filedatalake import DataLakeServiceClient\nfrom azure.core._match_conditions import MatchConditions\nfrom azure.storage.filedatalake._models import ContentSettings\nfrom azure.identity import DefaultAzureCredential, AzureCliCredential\n\nclass StorageClient:\n    def __init__(self, storage_account_name):\n        self.storage_account_name = storage_account_name\n        self.service_client = self.initialize_storage_account_ad()\n\n    def initialize_storage_account_ad(self):\n        try:          \n            credential = AzureCliCredential()\n            service_client = DataLakeServiceClient(account_url=f\"https://{self.storage_account_name}.dfs.core.windows.net\", credential=credential)\n\n            return service_client\n        except Exception as e:\n            print(e)\n            return\n\n    def download_file_from_directory(self, file_syst, directory, file_name):\n        try:\n            file_system_client = self.service_client.get_file_system_client(file_system=file_syst)\n            directory_client = file_system_client.get_directory_client(directory)\n            if not os.path.exists(\"downloaded_data\"):\n                os.makedirs(\"downloaded_data\")\n            print\n            local_file = open(f\"downloaded_data/{file_name}\",'wb')\n            file_client = directory_client.get_file_client(file_name)\n            download = file_client.download_file()\n            downloaded_bytes = download.readall()\n            local_file.write(downloaded_bytes)\n            local_file.close()\n            return\n        except Exception as e:\n            print(e)\n            return\n                          \n    def list_directory_contents(self, file_syst, directory):\n        try:\n            file_system_client = self.service_client.get_file_system_client(file_system=file_syst)\n\n            paths = file_system_client.get_paths(path=directory)\n\n            path_list = []\n            for path in paths:\n                path_list.append(path.name)\n            return path_list\n\n        except Exception as e:\n            print(e)\n            return\n                             \n    def create_directory(self, file_syst, directory):\n        try:\n            file_system_client = self.service_client.get_file_system_client(file_system=file_syst)\n            file_system_client.create_directory(directory)\n            return\n        except Exception as e:\n            print(e)\n            return\n    def upload_file_to_directory(self, file_syst, directory, uploaded_file_name, file_to_upload):\n        try:\n            file_system_client = self.service_client.get_file_system_client(file_system=file_syst)\n            directory_client = file_system_client.get_directory_client(directory)\n            file_client = directory_client.create_file(uploaded_file_name)\n            with open(file_to_upload, 'r') as local_file:\n                file_contents = local_file.read()\n                file_client.append_data(data=file_contents, offset=0, length=len(file_contents))\n                file_client.flush_data(len(file_contents))\n            return\n        except Exception as e:\n            print(e)\n            return\n\n\n\n\n\nclient = StorageClient(storage_account_name)\n\n\n\n\n\navailable_files = client.list_directory_contents(input_data_fs_name, data_directory_path)\nprint(available_files)\n\n\n\n\n\n[client.download_file_from_directory(input_data_fs_name, data_directory_path, datafile.rsplit(\"/\",1)[-1]) for datafile in available_files]\n\n\n\n\n\nclient.download_file_from_directory(input_data_fs_name, data_directory_path, available_files[0].rsplit(\"/\", 1)[-1])\n\n\n\n\n\nimport re\nimport pandas as pd\nparquet = []\ncsv = []\nfor x in available_files:\n    parquet_re = re.match(\"^.*\\.parquet\", x)\n    csv_re = re.match(\"^.*\\.csv\", x)\n    if parquet_re is not None:\n        parquet.append(parquet_re.group(0))\n    if csv_re is not None:\n        csv.append(csv_re.group(0))\n\n\n\n\nlocal_df = pd.read_parquet(f\"downloaded_data/{parquet[0].rsplit('/',1)[-1]}\")\nlocal_df.head()\n\n\n\n\n\nlocal_df = pd.read_csv(f\"downloaded_data/{csv[0].rsplit('/',1)[-1]}\")\nlocal_df.head()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "anatomy/vm.html",
    "href": "anatomy/vm.html",
    "title": "Virtual Machines",
    "section": "",
    "text": "As a user of FlowEHR you will need to create a virtual machine (VM) within an Azure Workspace in order to access FlowEHR assets within a secure Trusted Research Environment (TRE). The following instructions describe the process of VM creation and access, as well as some initial setup to access a Jupyter notebook once inside the VM.\nThis documentation relies heavily upon the official AzureTRE docs which should be consulted before performing the below steps."
  },
  {
    "objectID": "anatomy/vm.html#creating-a-new-vm",
    "href": "anatomy/vm.html#creating-a-new-vm",
    "title": "Virtual Machines",
    "section": "Creating a new VM",
    "text": "Creating a new VM\n\nVisit the FlowEHR Azure TRE landing page URL (please request from a member of the team)\nSelect an appropriate Workspace from the list provided\nUnder Workspace Services, select the VMs option\nUnder Resources, you will see any previously created VMs. If there are none, select Create New in the upper right\nSelect Linux Machine > Create (only option available as of 05/12/2022)\nChoose an approriate name and description for the VM and select the desired image.\n\nFor linux VMs, there are two Ubuntu 18.04 images available. The Data Science variant may have a more relevant selection of packages installed.\n\nSelect an approriate VM size from the dropdown menu. If you’re not sure of your requirements, opt for the 2 CPU | 8GB RAM option. You can scale this up later if needed.\nCheck the box marked Shared storage to enable access to workspace shared storage.\nHit Submit. Your VM will start provisioning and you may need to wait a few minutes."
  },
  {
    "objectID": "anatomy/vm.html#stopping-a-running-vm",
    "href": "anatomy/vm.html#stopping-a-running-vm",
    "title": "Virtual Machines",
    "section": "Stopping a running VM",
    "text": "Stopping a running VM\n\nSelect the three dots at the upper right of the VM entry under Resources\nSelect Action > Stop\nThe VM will be stopped momentarily"
  },
  {
    "objectID": "anatomy/vm.html#starting-a-stopped-vm",
    "href": "anatomy/vm.html#starting-a-stopped-vm",
    "title": "Virtual Machines",
    "section": "Starting a stopped VM",
    "text": "Starting a stopped VM\n\nSelect the three dots at the upper right of the VM entry under Resources\nSelect Action > Start"
  },
  {
    "objectID": "anatomy/vm.html#connecting-to-a-created-vm",
    "href": "anatomy/vm.html#connecting-to-a-created-vm",
    "title": "Virtual Machines",
    "section": "Connecting to a created VM",
    "text": "Connecting to a created VM\n\nVisit the FlowEHR Azure TRE landing page URL\nSelect an appropriate Workspace from the list provided\nUnder Workspace Services, select the VMs option\nUnder Resources, you will see any previously created VMs\nSelect Connect under the VM you wish to connect to. This launches a separate browser window or tab."
  },
  {
    "objectID": "anatomy/vm.html#deleting-a-vm",
    "href": "anatomy/vm.html#deleting-a-vm",
    "title": "Virtual Machines",
    "section": "Deleting a VM",
    "text": "Deleting a VM\n\nSelect the three dots at the upper right of the VM entry under Resources\nSelect Action > Stop\nWhen the VM has been deallocated, repeat step 1. and select Delete"
  },
  {
    "objectID": "anatomy/vm.html#installing-software-within-the-vm",
    "href": "anatomy/vm.html#installing-software-within-the-vm",
    "title": "Virtual Machines",
    "section": "Installing software within the VM",
    "text": "Installing software within the VM\nAs the VM is located within a TRE, most outbound internet access is restricted.\n\nGitHub\nAccess to repositories hosted on https://github.com/ is restricted. Repositories can be mirrored on an ad-hoc basis to a TRE-accessible Gitea instance, where this documentation can also be found.\nYou should be supplied with the URL for the Gitea instance during your onboarding. If you have not received the Gitea URL, please request it from member of the team.\n\n\nAPT packages\nThe VM has access to software packages via a Nexus mirror\n\n\nPython packages via Conda and PyPI\nThe Nexus mirror provides mirrored access to packages available via conda forge and PyPI via the standard pip install and conda install command line interfaces."
  },
  {
    "objectID": "anatomy/vm.html#example-running-a-jupyter-notebook-from-this-documentation-within-a-virtual-environment",
    "href": "anatomy/vm.html#example-running-a-jupyter-notebook-from-this-documentation-within-a-virtual-environment",
    "title": "Virtual Machines",
    "section": "Example: Running a Jupyter notebook from this documentation within a virtual environment",
    "text": "Example: Running a Jupyter notebook from this documentation within a virtual environment\nWithin a TRE VM:\n\nVisit the Gitea URL provided alongside this documentation\nSelect ‘Explore’ from the top left\nSelect the ‘End-User-Docs’ repository\nCopy the HTTPS link to the repository\n\nThe following code\n\nClones this documentation repo\nCreates a virtual environment\nActivates the environment\nInstalls jupyterlab and ipykernel within the virtual environment\n\nMakes the virtual environment available as a kernel within a Jupyter Lab environment\nLaunches Jupyter Lab\n\nOpen a terminal within the VM and create a virtual environment with Conda\nconda create -n my_virtual_environment python=3.10\nconda activate my_virtual_environment\nYou may be prompted by conda to initialise your shell. Run\nconda init bash\nand restart your terminal. Run the following commands in the restarted terminal to clone this documentation repo and install dependencies\ngit clone ${gitea-repository-url}\ncd End-User-Docs/docs/notebooks\nconda create -n my_virtual_environment python=3.10\nconda activate my_virtual_environment\nconda install jupyterlab ipykernel\npip install -r requirements.txt\npython -m ipykernel install --user --name=my_env\nA Jupyter Lab environment can then be launched by running\njupyter lab\nWithin this environment, navigate to the example notebooks provided. When running the notebooks, ensure the python kernel is set to the name defined by the earlier python -m ipykernel install command."
  },
  {
    "objectID": "anatomy/vm.html#accessing-ehr-and-dicom-data-within-the-tre",
    "href": "anatomy/vm.html#accessing-ehr-and-dicom-data-within-the-tre",
    "title": "Virtual Machines",
    "section": "Accessing EHR and DICOM data within the TRE",
    "text": "Accessing EHR and DICOM data within the TRE\nThis documentation is accompanied by Jupyter notebook files which provide detailed examples of accessing and viewing EHR and DICOM data from within a Jupyter Lab environment within a TRE VM.\nBefore launching the notebooks and after following the steps in the above example, install required dependencies within your created virtual environment with\npip install -r requirements.txt\n\nA note on the data accessible from the TRE\nData provided has gone through an anonymisation step and will not entirely look like the original data due to the removal of PII and consequent structural changes that may appear as a result, such as line break removals.\nAs such, structural elements of reports such as line breaks or sentence lengths should not be used to generate features as inputs to machine learning models."
  },
  {
    "objectID": "anatomy/tre.html",
    "href": "anatomy/tre.html",
    "title": "Trusted Research Environment",
    "section": "",
    "text": "The FlowEHR TRE (Trusted Research Environment) provides a safe and secure environment to run clinical research on real data generated in clinical settings. The FlowEHR TRE builds upon the work of the Microsoft Azure TRE to provide access to cloud-scale computing environments to conduct real-world Machine Learning experiments from initial inception through to operational deployment.\nTRE Workspaces are designed to be project-specific, with access to the Workspace resources and Workspace data restricted to the users assigned to the Workspace. Users from other Workspaces will not be able to see or interact with your Workspace resources or data."
  },
  {
    "objectID": "anatomy/tre.html#getting-started",
    "href": "anatomy/tre.html#getting-started",
    "title": "Trusted Research Environment",
    "section": "Getting Started",
    "text": "Getting Started\nYour TRE administrator will provide you with the following information: 1) Account details to logon and access TRE resources 2) The address for the TRE Portal 3) One or more Workspace IDs"
  },
  {
    "objectID": "anatomy/tre.html#the-tre-landing-page",
    "href": "anatomy/tre.html#the-tre-landing-page",
    "title": "Trusted Research Environment",
    "section": "The TRE Landing Page",
    "text": "The TRE Landing Page\nTRE Workspaces are deployed to a virtual private network and the resources available in the Workspace are designed to have no direct internet access and are not accessible from the internet either.\nTo get access to your TRE Workspace, you must first logon to the TRE portal. Your TRE administrator will provide you with the URL to access the portal along with a logon id that grants you access to Workspace resources.\nOnce logged-in, you will see the TRE Landing Page:\n\nThe main components of the Landing Page are:\n\nThe TRE Header\nThe TRE Navigation Side-Panel\nThe ‘Workspaces’ Pane\nThe TRE Footer\n\n\nThe TRE Header\n\nThe TRE Header contains the following links:\n\nThe home link - a half-full (we’re optimists) beaker named ‘Azure TRE’. Use this link to return to the Landing Page from any location within the TRE application.\nA notifications bell - an icon which can be used for quick access to any notifications you may have\nAn account link - you can use this link to logout of the TRE\n\n\n\nThe Navigation Side-Panel\n\nThe Navigation Side-Panel provides context-sensitive navigation within the TRE application.\n\n\nThe Workspace Pane\n\nThe ‘Workspaces’ Pane contains Card(s) for the Workspaces that you have been granted access to in the TRE.\nEach Card displays:\n\nThe Workspace Name\nDescription for the Workspace\nAn information button\nA Cost-Notifier button: displays the accumulated costs for the workspace\n\nClicking the Workspace Name or a blank space on a Workspace Card will connect you to the Workspace."
  },
  {
    "objectID": "anatomy/tre.html#using-a-workspace",
    "href": "anatomy/tre.html#using-a-workspace",
    "title": "Trusted Research Environment",
    "section": "Using a Workspace",
    "text": "Using a Workspace\nOnce connected to a workspace, you’ll be able to see the services installed in the Workspace and also the Shared Services available to all Workspaces.\n\nThe ‘Create New’ button can be used to add services to the Workspace. Only TRE Administrators or Workspace Owners can add Services to a Workspace.\nHowever, certain Workspace Services provide User Resources which enable all workspace users to add these resources to the Workspace. Specifically, the Virtual Machines Service provides a Virtual Machine User Resource. Once the Workspace Owner has added a Virtual Machines Service to the Workspace, users can connect to the service and add Virtual Machines for their personal use.\nFrom the Workspace Overview page, you can find important information about your workspace by clicking the details tab. One key piece of information here is the ‘Workspace id’. This is a four-character code, that is appended to the names of the resources deployed in your workspace. Workspace ID can be useful to know when you are connecting to resources such as AMLS\n\nAccess a Virtual Machine\nLook for the Virtual Machines or Virtual Desktops Service in the Services section of your workspace:\n\nAvoid the ‘connect’ button and instead click on the title of the Virtual Machines Service Card. This will take you to a page showing all the Virtual Machines that you have access to:\n\nThe Virtual Machines in your Workspace allow you to interact with the private resources in the Workspace and with the Shared Services common to all workspaces. When you connect to a VM that you have created, you will be automatically logged-in with a user account with administrator rights. So, on a Linux VM, you will be able to run privileged commands via sudo.\nDetailed instructions for using a VM can be found in the Accessing Virtual Machines document\n\n\nAzure Machine Learning Services (AMLS)\nThe Azure Machine Learning Services in your workspace are provisioned in a private virtual network - there is no direct access to the service over the internet. Because the Virtual Machines are deployed to the same private network as AMLS, you can connect to AMLS from within a Virtual Machine.\nYou’ll need to know the URL to use for connecting. Click on the Azure Machine Learning service in your workspace then select the details tab. Towards the bottom left of this page you will see the value for the ‘internal connection url’ which you can use to access the AMLS service from your Virtual Machine.\n\nAn alternative to copy and pasting this URL, involves using your Workspace ID (which you can find in the via Workspace > Overview > Details). Having logged-on to your workspace VM, navigate to ‘https://ml.azure.com’ and sign-in using the same credentials that you use to access the TRE. Then navigate to ‘Workspaces’ and select the workspace corresponding to your TRE Workspace ID (the resource group for the workspace contains the workspace id as its last four characters):"
  },
  {
    "objectID": "anatomy/tre.html#shared-services",
    "href": "anatomy/tre.html#shared-services",
    "title": "Trusted Research Environment",
    "section": "Shared Services",
    "text": "Shared Services\nShared Services provide access to resources that you would otherwise need to access over the internet. Shared Services include:\n\nGitea\nNexus\nFirewall\n\nThe Shared Services are available for use by all workspaces, but their configuration can only be changed by the TRE administrators\n\nGitea\nGitea is used in the TRE to mirror code repositories from the internet. You can browse the repositories available in Gitea from your Workspace VM using the URL https://gitea-[TRE-ID].azurewebsites.net. (Check with your TRE administrator for the correct URL)\nContact your TRE administrator if you need to add a git repository to the Gitea service.\nGitea is also available as a Workspace service, this is useful if you wish to mirror a private repository, without making this accessible from other Workspaces.\nWhilst Gitea is used to mirror repositories from the internet, for security reasons, the server does not allow you to push changes to internet sources.\n\n\nNexus\nNexus is used in the TRE to provide a package mirror for popular software tools. Installation tools on your VMs are configured to pull from the Nexus server rather than directly from the internet.\nYou can explore the available repositories mirrored by the Nexus server from your VM using https://nexus-[TRE-ID].[LOCATION].cloudapp.azure.com. (Check with your TRE administrator for the correct URL)\nClick on the ‘Browse’ link to see the full list of repositories that are in the Nexus mirror.\n\nIf you need to configure an application to use a nexus repo, you can click the ‘copy’ button in the URL column for the mirror that you are interested in. Then you can set your application to use this mirror.\nFor instance, if you want to configure python to use the pypi mirror on the Nexus server you could type:\npip config --user set global.index https://nexus-[TRE-ID].[LOCATION].cloudapp.azure.com/repository/pypi/pypi\npip config --user set global.index-url https://nexus-[TRE-ID].[LOCATION].cloudapp.azure.com/pypi/simple\npip config --user set global.trusted-host nexus-[TRE-ID].[LOCATION].cloudapp.azure.com\nFor Python, you may find that this has already been setup on your VM. You can check this by running:\npip config list\nSimilarly, on a Linux VM, the VM will be configured to use the apt repositories from Nexus. If you run sudo apt update, you will see that apt will only search for updates from the Nexus server."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Hitch-hiker’s guide to FlowEHR",
    "section": "",
    "text": "FlowEHR is a development and deployment platform for digital research and innovation for healthcare providers. It provides technical controls to protect privacy but an smooth pathway for deployment of data driven products to the clinic or the bedside. Whilst it is a platform to bridge the AI chasm for health, we anticipate health data researchers, application developers, and clinical analysts to accompany us on our journey.\nOur aim is …\nThese are principles are sometimes framed as barries but only by full engaging with each of them will it be possible to use data to improve the quality and safety of health care. When combined we believe this will enable the deployment of digital tools to solve operational and clinical problems.\nOur original use case was the safe & effective development & deployment of real-time translational Machine Learning interventions into clinical and operational settings inside a busy NHS hospital."
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "The Hitch-hiker’s guide to FlowEHR",
    "section": "Next steps",
    "text": "Next steps\nWe would recommend using the QuickStart guide to follow and ‘end-to-end’ walkthrough of an application from concept to implementation. This will begin with deploying the FlowEHR infrastructure for personal use with synthetic data, and end with inspecting the performance of an algorithm in the (simulated) wild.\nSpecific users may wish to dive directly into documentation relevant to their role. This includes\n\nthe data scientist for those providing analytical insights or building models against live data\nthe application developer for those building and deploying applications\nthe platform maintainer for those responsible for installing and mananaging the platform\nthe algorithm steward for those using the platform to monitor, audit and observe data driven applications in use"
  }
]